
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="Cars, Computer, and Code.">
      
      
        <meta name="author" content="XtremeOwnage">
      
      
        <link rel="canonical" href="https://Static.XtremeOwnage.com/blog/2024/proxmox---reinstall-host-without-losing-cluster-configuration/">
      
      
        <link rel="prev" href="../backup-strategies/">
      
      
        <link rel="next" href="../proxmox---debian-cloud-init-templates/">
      
      
        
      
      
        <link rel="alternate" type="application/rss+xml" title="RSS feed" href="../../../feed_rss_created.xml">
        <link rel="alternate" type="application/rss+xml" title="RSS feed of updated content" href="../../../feed_rss_updated.xml">
      
      <link rel="icon" href="../../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.14+insiders-4.53.16">
    
    
      
        <title>Proxmox - Reinstall host without losing cluster configuration - XtremeOwnage.com</title>
      
    
    
      <link rel="stylesheet" href="../../../assets/stylesheets/main.8cce3a28.min.css">
      
        
        <link rel="stylesheet" href="../../../assets/stylesheets/palette.ab4e12ef.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Open Sans";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../../../stylesheets/extra.css">
    
    <script>__md_scope=new URL("../../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
   <link href="../../../assets/stylesheets/glightbox.min.css" rel="stylesheet"/><style>
    html.glightbox-open { overflow: initial; height: 100%; }
    .gslide-title { margin-top: 0px; user-select: text; }
    .gslide-desc { color: #666; user-select: text; }
    .gslide-image img { background: white; }
    .gscrollbar-fixer { padding-right: 15px; }
    .gdesc-inner { font-size: 0.75rem; }
    body[data-md-color-scheme="slate"] .gdesc-inner { background: var(--md-default-bg-color);}
    body[data-md-color-scheme="slate"] .gslide-title { color: var(--md-default-fg-color);}
    body[data-md-color-scheme="slate"] .gslide-desc { color: var(--md-default-fg-color);}</style> <script src="../../../assets/javascripts/glightbox.min.js"></script></head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="grey" data-md-color-accent="orange">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#proxmox-howto-restore-pve-host-configuration" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      <!--
  Copyright (c) 2016-2022 Martin Donath <martin.donath@squidfunk.com>

  Permission is hereby granted, free of charge, to any person obtaining a copy
  of this software and associated documentation files (the "Software"), to
  deal in the Software without restriction, including without limitation the
  rights to use, copy, modify, merge, publish, distribute, sublicense, and/or
  sell copies of the Software, and to permit persons to whom the Software is
  furnished to do so, subject to the following conditions:

  The above copyright notice and this permission notice shall be included in
  all copies or substantial portions of the Software.

  THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
  IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
  FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT. IN NO EVENT SHALL THE
  AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
  LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
  FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS
  IN THE SOFTWARE.
-->

<!-- Determine base classes -->





<!-- Header -->
<header class="md-header md-header--lifted" data-md-component="header">
    <nav class="md-header__inner md-grid" aria-label="header.title">

        <!-- Link to home -->
        <a href="../../.." title="XtremeOwnage.com"
            class="md-header__button md-logo" aria-label="XtremeOwnage.com" data-md-component="logo">
            
  <img src="/assets/avatar/XO.png" alt="logo">

        </a>

        <!-- Button to open drawer -->
        <label class="md-header__button md-icon" for="__drawer">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
        </label>

        <!-- Header title -->
        <div class="md-header__title" data-md-component="header-title">
            <div class="md-header__ellipsis">
                <div class="md-header__topic">
                    <span class="md-ellipsis">
                        XtremeOwnage.com
                    </span>
                </div>
                <div class="md-header__topic" data-md-component="header-topic">
                    <span class="md-ellipsis">
                        
                        Proxmox - Reinstall host without losing cluster configuration
                        
                    </span>
                </div>
            </div>
        </div>

        <!-- Color palette -->
        
        <form class="md-header__option" data-md-component="palette">
            
            
            
            
            <input class="md-option" data-md-color-media="(prefers-color-scheme: light)"
                data-md-color-scheme="default"
                data-md-color-primary="grey"
                data-md-color-accent="orange"  aria-hidden="true"  type="radio"
                name="__palette" id="__palette_0" />
            
            
        </form>
        



        <!-- Navigation tabs (sticky) -->
        
        
        
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="../../.." class="md-tabs__link">
        
  
  
    
  
  Home

      </a>
    </li>
  

      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="../../../pages/Projects/" class="md-tabs__link">
        
  
  
    
  
  Projects

      </a>
    </li>
  

      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="../../../pages/tags/" class="md-tabs__link">
        
  
  
    
  
  Tags

      </a>
    </li>
  

      
        
  
  
  
    
  
  
    
    
      <li class="md-tabs__item md-tabs__item--active">
        <a href="../../" class="md-tabs__link">
          
  
  
    
  
  Blog

        </a>
      </li>
    
  

      
    </ul>
  </div>
</nav>
        
        

        <!-- User preference: color palette -->
        
        <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
        

        <div class="header-social">
            
            <a href="https://github.com/XtremeOwnageDotCom" target="_blank" rel="noopener" title="" class="md-social__link">
                <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><!--! Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8M97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg>
            </a>
            
            <a href="/discord" target="_blank" rel="noopener" title="" class="md-social__link">
                <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 640 512"><!--! Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M524.531 69.836a1.5 1.5 0 0 0-.764-.7A485 485 0 0 0 404.081 32.03a1.82 1.82 0 0 0-1.923.91 338 338 0 0 0-14.9 30.6 447.9 447.9 0 0 0-134.426 0 310 310 0 0 0-15.135-30.6 1.89 1.89 0 0 0-1.924-.91 483.7 483.7 0 0 0-119.688 37.107 1.7 1.7 0 0 0-.788.676C39.068 183.651 18.186 294.69 28.43 404.354a2.02 2.02 0 0 0 .765 1.375 487.7 487.7 0 0 0 146.825 74.189 1.9 1.9 0 0 0 2.063-.676A348 348 0 0 0 208.12 430.4a1.86 1.86 0 0 0-1.019-2.588 321 321 0 0 1-45.868-21.853 1.885 1.885 0 0 1-.185-3.126 251 251 0 0 0 9.109-7.137 1.82 1.82 0 0 1 1.9-.256c96.229 43.917 200.41 43.917 295.5 0a1.81 1.81 0 0 1 1.924.233 235 235 0 0 0 9.132 7.16 1.884 1.884 0 0 1-.162 3.126 301.4 301.4 0 0 1-45.89 21.83 1.875 1.875 0 0 0-1 2.611 391 391 0 0 0 30.014 48.815 1.86 1.86 0 0 0 2.063.7A486 486 0 0 0 610.7 405.729a1.88 1.88 0 0 0 .765-1.352c12.264-126.783-20.532-236.912-86.934-334.541M222.491 337.58c-28.972 0-52.844-26.587-52.844-59.239s23.409-59.241 52.844-59.241c29.665 0 53.306 26.82 52.843 59.239 0 32.654-23.41 59.241-52.843 59.241m195.38 0c-28.971 0-52.843-26.587-52.843-59.239s23.409-59.241 52.843-59.241c29.667 0 53.307 26.82 52.844 59.239 0 32.654-23.177 59.241-52.844 59.241"/></svg>
            </a>
            
            <a href="https://www.reddit.com/r/HTTP_404_NotFound" target="_blank" rel="noopener" title="" class="md-social__link">
                <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M0 256C0 114.6 114.6 0 256 0s256 114.6 256 256-114.6 256-256 256H37.1c-13.7 0-20.5-16.5-10.9-26.2L75 437C28.7 390.7 0 326.7 0 256m349.6-102.4c23.6 0 42.7-19.1 42.7-42.7s-19.1-42.7-42.7-42.7c-20.6 0-37.8 14.6-41.8 34-34.5 3.7-61.4 33-61.4 68.4v.2c-37.5 1.6-71.8 12.3-99 29.1-10.1-7.8-22.8-12.5-36.5-12.5-33 0-59.8 26.8-59.8 59.8 0 24 14.1 44.6 34.4 54.1 2 69.4 77.6 125.2 170.6 125.2s168.7-55.9 170.6-125.3c20.2-9.6 34.1-30.2 34.1-54 0-33-26.8-59.8-59.8-59.8-13.7 0-26.3 4.6-36.4 12.4-27.4-17-62.1-27.7-100-29.1v-.2c0-25.4 18.9-46.5 43.4-49.9 4.4 18.8 21.3 32.8 41.5 32.8zm-172.5 93.3c16.7 0 29.5 17.6 28.5 39.3s-13.5 29.6-30.3 29.6-31.4-8.8-30.4-30.5S160.3 247 177 247zm190.1 38.3c1 21.7-13.7 30.5-30.4 30.5s-29.3-7.9-30.3-29.6 11.8-39.3 28.5-39.3 31.2 16.6 32.1 38.3zm-48.1 56.7c-10.3 24.6-34.6 41.9-63 41.9s-52.7-17.3-63-41.9c-1.2-2.9.8-6.2 3.9-6.5 18.4-1.9 38.3-2.9 59.1-2.9s40.7 1 59.1 2.9c3.1.3 5.1 3.6 3.9 6.5"/></svg>
            </a>
            
            <a href="https://static.xtremeownage.com/" target="_blank" rel="noopener" title="" class="md-social__link">
                <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M352 256c0 22.2-1.2 43.6-3.3 64H163.4c-2.2-20.4-3.3-41.8-3.3-64s1.2-43.6 3.3-64h185.3c2.2 20.4 3.3 41.8 3.3 64m28.8-64h123.1c5.3 20.5 8.1 41.9 8.1 64s-2.8 43.5-8.1 64H380.8c2.1-20.6 3.2-42 3.2-64s-1.1-43.4-3.2-64m112.6-32H376.7c-10-63.9-29.8-117.4-55.3-151.6 78.3 20.7 142 77.5 171.9 151.6zm-149.1 0H167.7c6.1-36.4 15.5-68.6 27-94.7 10.5-23.6 22.2-40.7 33.5-51.5C239.4 3.2 248.7 0 256 0s16.6 3.2 27.8 13.8c11.3 10.8 23 27.9 33.5 51.5 11.6 26 20.9 58.2 27 94.7m-209 0H18.6c30-74.1 93.6-130.9 172-151.6-25.5 34.2-45.3 87.7-55.3 151.6M8.1 192h123.1c-2.1 20.6-3.2 42-3.2 64s1.1 43.4 3.2 64H8.1C2.8 299.5 0 278.1 0 256s2.8-43.5 8.1-64m186.6 254.6c-11.6-26-20.9-58.2-27-94.6h176.6c-6.1 36.4-15.5 68.6-27 94.6-10.5 23.6-22.2 40.7-33.5 51.5-11.2 10.7-20.5 13.9-27.8 13.9s-16.6-3.2-27.8-13.8c-11.3-10.8-23-27.9-33.5-51.5zM135.3 352c10 63.9 29.8 117.4 55.3 151.6-78.4-20.7-142-77.5-172-151.6zm358.1 0c-30 74.1-93.6 130.9-171.9 151.6 25.5-34.2 45.2-87.7 55.3-151.6h116.7z"/></svg>
            </a>
            
            <a href="/feed_rss_created.xml" target="_blank" rel="noopener" title="" class="md-social__link">
                <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M0 64c0-17.7 14.3-32 32-32 229.8 0 416 186.2 416 416 0 17.7-14.3 32-32 32s-32-14.3-32-32C384 253.6 226.4 96 32 96 14.3 96 0 81.7 0 64m0 352a64 64 0 1 1 128 0 64 64 0 1 1-128 0m32-256c159.1 0 288 128.9 288 288 0 17.7-14.3 32-32 32s-32-14.3-32-32c0-123.7-100.3-224-224-224-17.7 0-32-14.3-32-32s14.3-32 32-32"/></svg>
            </a>
            
        </div>

        <!-- Button to open search modal -->
        
        <label class="md-header__button md-icon" for="__search">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        </label>

        <!-- Search interface -->
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
          <a href="javascript:void(0)" class="md-search__icon md-icon" title="Share" aria-label="Share" data-clipboard data-clipboard-text="" data-md-component="search-share" tabindex="-1">
            
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M18 16.08c-.76 0-1.44.3-1.96.77L8.91 12.7c.05-.23.09-.46.09-.7s-.04-.47-.09-.7l7.05-4.11c.54.5 1.25.81 2.04.81a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3c0 .24.04.47.09.7L8.04 9.81C7.5 9.31 6.79 9 6 9a3 3 0 0 0-3 3 3 3 0 0 0 3 3c.79 0 1.5-.31 2.04-.81l7.12 4.15c-.05.21-.08.43-.08.66 0 1.61 1.31 2.91 2.92 2.91s2.92-1.3 2.92-2.91A2.92 2.92 0 0 0 18 16.08"/></svg>
          </a>
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
        <div class="md-search__suggest" data-md-component="search-suggest"></div>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
        

        <!-- Repository information -->
        
    </nav>
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
                
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" hidden>
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../../.." title="XtremeOwnage.com" class="md-nav__button md-logo" aria-label="XtremeOwnage.com" data-md-component="logo">
      
  <img src="/assets/avatar/XO.png" alt="logo">

    </a>
    XtremeOwnage.com
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../.." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Home
  

    
  </span>
  
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../pages/Projects/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Projects
  

    
  </span>
  
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../pages/tags/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Tags
  

    
  </span>
  
  

      </a>
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
      
        
          
        
      
        
      
        
      
    
    
    
      
        
        
      
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4" checked>
        
          
          <div class="md-nav__link md-nav__container">
            <a href="../../" class="md-nav__link ">
              
  
  
  <span class="md-ellipsis">
    
  
    Blog
  

    
  </span>
  
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_4" id="__nav_4_label" tabindex="">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_4">
            <span class="md-nav__icon md-icon"></span>
            
  
    Blog
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    
    
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_4_2" >
        
          
          <label class="md-nav__link" for="__nav_4_2" id="__nav_4_2_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Archive
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_4_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4_2">
            <span class="md-nav__icon md-icon"></span>
            
  
    Archive
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
    
  
  
    <li class="md-nav__item">
      <a href="../../archive/2025/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    2025
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    
  
  
    <li class="md-nav__item">
      <a href="../../archive/2024/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    2024
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    
  
  
    <li class="md-nav__item">
      <a href="../../archive/2023/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    2023
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    
  
  
    <li class="md-nav__item">
      <a href="../../archive/2022/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    2022
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    
  
  
    <li class="md-nav__item">
      <a href="../../archive/2021/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    2021
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    
  
  
    <li class="md-nav__item">
      <a href="../../archive/2020/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    2020
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    
  
  
    <li class="md-nav__item">
      <a href="../../archive/2019/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    2019
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_4_3" >
        
          
          <label class="md-nav__link" for="__nav_4_3" id="__nav_4_3_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Categories
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_4_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4_3">
            <span class="md-nav__icon md-icon"></span>
            
  
    Categories
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
    
  
  
    <li class="md-nav__item">
      <a href="../../category/development/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Development
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    
  
  
    <li class="md-nav__item">
      <a href="../../category/home-automation/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Home-Automation
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    
  
  
    <li class="md-nav__item">
      <a href="../../category/home-improvement/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Home-Improvement
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    
  
  
    <li class="md-nav__item">
      <a href="../../category/kubernetes/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Kubernetes
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    
  
  
    <li class="md-nav__item">
      <a href="../../category/misc/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Misc
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    
  
  
    <li class="md-nav__item">
      <a href="../../category/shop-talk/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Shop-Talk
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    
  
  
    <li class="md-nav__item">
      <a href="../../category/solar/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Solar
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    
  
  
    <li class="md-nav__item">
      <a href="../../category/technology/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Technology
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
                
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#what-happened" class="md-nav__link">
    <span class="md-ellipsis">
      
        What happened?
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#how-to-restore-the-host" class="md-nav__link">
    <span class="md-ellipsis">
      
        How to restore the host
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="How to restore the host">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#first-reinstall-proxmox" class="md-nav__link">
    <span class="md-ellipsis">
      
        First- Reinstall Proxmox.
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#restoring-the-configuration" class="md-nav__link">
    <span class="md-ellipsis">
      
        Restoring the configuration
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Restoring the configuration">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#step-1-prepare-remote-access-to-a-working-machine" class="md-nav__link">
    <span class="md-ellipsis">
      
        Step 1. Prepare remote access to a working machine.
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#step-2-completely-stop-proxmox" class="md-nav__link">
    <span class="md-ellipsis">
      
        Step 2. COMPLETELY stop Proxmox
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#step-3-prepare-etcpve" class="md-nav__link">
    <span class="md-ellipsis">
      
        Step 3. Prepare /etc/pve
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#step-4-corosync" class="md-nav__link">
    <span class="md-ellipsis">
      
        Step 4. Corosync
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#step-5-pve-cluster" class="md-nav__link">
    <span class="md-ellipsis">
      
        Step 5. PVE-Cluster
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#step-6-reboot" class="md-nav__link">
    <span class="md-ellipsis">
      
        Step 6. Reboot
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#step-7-reimport-zfs-pools" class="md-nav__link">
    <span class="md-ellipsis">
      
        Step 7. Reimport ZFS Pools
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#step-8-reimport-ceph-osds" class="md-nav__link">
    <span class="md-ellipsis">
      
        Step 8. Reimport Ceph OSDs
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Step 8. Reimport Ceph OSDs">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#gathering-information" class="md-nav__link">
    <span class="md-ellipsis">
      
        Gathering information
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#re-import-the-lvms" class="md-nav__link">
    <span class="md-ellipsis">
      
        Re-import the LVMs!
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#step-9-re-import-sdn" class="md-nav__link">
    <span class="md-ellipsis">
      
        Step 9. Re-import SDN
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#step-10-regenerate-certs-and-remove-old-ssh-keys" class="md-nav__link">
    <span class="md-ellipsis">
      
        Step 10. Regenerate certs, and remove old ssh keys
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Step 10. Regenerate certs, and remove old ssh keys">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#ansible-task-to-re-link-symlinks" class="md-nav__link">
    <span class="md-ellipsis">
      
        Ansible task to re-link symlinks
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#summary" class="md-nav__link">
    <span class="md-ellipsis">
      
        Summary
      
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
  <div class="md-content md-content--post" data-md-component="content">
    <div class="md-sidebar md-sidebar--post" data-md-component="sidebar" data-md-type="navigation">
      <div class="md-sidebar__scrollwrap">
        <div class="md-sidebar__inner md-post">
          <nav class="md-nav md-nav--primary">
            <div class="md-post__back">
              <div class="md-nav__title md-nav__container">
                <a href="../../" class="md-nav__link">
                  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
                  <span class="md-ellipsis">
                    Back to index
                  </span>
                </a>
              </div>
            </div>
            
              <div class="md-post__authors md-typeset">
                
                  <div class="md-profile md-post__profile">
                    <span class="md-author md-author--long">
                      <img src="/assets/avatar/XO.png" alt="XtremeOwnage">
                    </span>
                    <span class="md-profile__description">
                      <strong>
                        
                          XtremeOwnage
                        
                      </strong>
                      <br>
                      Creator
                    </span>
                  </div>
                
              </div>
            
            <ul class="md-post__meta md-nav__list">
              <li class="md-nav__item md-nav__item--section">
                <div class="md-post__title">
                  <span class="md-ellipsis">
                    Metadata
                  </span>
                </div>
                <nav class="md-nav">
                  <ul class="md-nav__list">
                    <li class="md-nav__item">
                      <div class="md-nav__link">
                        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 19H5V8h14m-3-7v2H8V1H6v2H5c-1.11 0-2 .89-2 2v14a2 2 0 0 0 2 2h14a2 2 0 0 0 2-2V5a2 2 0 0 0-2-2h-1V1m-1 11h-5v5h5z"/></svg>
                        <time datetime="2024-06-12 00:00:00+00:00" class="md-ellipsis">Jun 12, 2024</time>
                      </div>
                    </li>
                    
                    
                      <li class="md-nav__item">
                        <div class="md-nav__link">
                          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9 3v15h3V3zm3 2 4 13 3-1-4-13zM5 5v13h3V5zM3 19v2h18v-2z"/></svg>
                          <span class="md-ellipsis">
                            in
                            
                              <a href="../../category/technology/">Technology</a></span>
                        </div>
                      </li>
                    
                    
                      
                      <li class="md-nav__item">
                        <div class="md-nav__link">
                          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 20a8 8 0 0 0 8-8 8 8 0 0 0-8-8 8 8 0 0 0-8 8 8 8 0 0 0 8 8m0-18a10 10 0 0 1 10 10 10 10 0 0 1-10 10C6.47 22 2 17.5 2 12A10 10 0 0 1 12 2m.5 5v5.25l4.5 2.67-.75 1.23L11 13V7z"/></svg>
                          <span class="md-ellipsis">
                            
                              27 min read
                            
                          </span>
                        </div>
                      </li>
                    
                  </ul>
                </nav>
              </li>
            </ul>
            
          </nav>
          
        </div>
      </div>
    </div>
    <article class="md-content__inner md-typeset">
      
        
  


  <nav class="md-tags" >
    
      
      
      
      
        <a href="../../../pages/tags/#tag:homelab" class="md-tag">Homelab</a>
      
    
      
      
      
      
        <a href="../../../pages/tags/#tag:proxmox" class="md-tag">Proxmox</a>
      
    
  </nav>



<h1 id="proxmox-howto-restore-pve-host-configuration">Proxmox - Howto: Restore PVE Host Configuration<a class="headerlink" href="#proxmox-howto-restore-pve-host-configuration" title="Permanent link">&para;</a></h1>
<p>So, literally ONE DAY after writing <a href="../backup-strategies/" target="_blank">My backup strategies post</a>, I encountered a need to completely reinstall proxmox.</p>
<p>As it turns out, there is not a documented, nor official restore process. After some trial and error, I did manage to completely restore my host, with no loss in configuration.</p>
<!-- more -->

<h2 id="what-happened">What happened?<a class="headerlink" href="#what-happened" title="Permanent link">&para;</a></h2>
<p>So- here I was minding my own business, and all of a sudden- my entire network goes Kaboom. </p>
<p>Kubernetes is down, DNS is down, Home Assistant is down... and, very little was working correctly.</p>
<p>After running my trusty <a href="https://www.nirsoft.net/utils/multiple_ping_tool.html" target="_blank">PingInfoView</a>, which I have configured with the IP addresses of my infrastructure- I noticed my r730XD was completely offline.</p>
<p>After accessing it with iDrac, and opening the remote console- I discovered... It was unable to boot. Its boot NVMe was officially shot.</p>
<p>After opening one of my other proxmox nodes by IP, I also discovered, well. basically everything was offline, as my ceph cluster had become unavailable.</p>
<p>The reasoning for this- Apparently I did not have enough MONs to create a quorum, and when the host went down, ceph went down with it.</p>
<p>That- was its own fun to correct- but, that isn't the purpose of this post. This post, is about reinstalling a proxmox server, and just having it come back online magically, without losing any configuration.</p>
<div class="admonition info">
<p class="admonition-title">Info</p>
<p>This article ASSUMEs you have a cluster with at least one other healthy machine.</p>
<p>If not, this post is not for you.</p>
</div>
<p>Per my <a href="../backup-strategies/" target="_blank">post on backups</a>, I do not do full image level backups for these hosts, as you can reinstall them, and just reconfigure them.</p>
<p>But, I do, do configuration backups of /etc/pve, as this drive holds the cluster state.</p>
<h2 id="how-to-restore-the-host">How to restore the host<a class="headerlink" href="#how-to-restore-the-host" title="Permanent link">&para;</a></h2>
<h3 id="first-reinstall-proxmox">First- Reinstall Proxmox.<a class="headerlink" href="#first-reinstall-proxmox" title="Permanent link">&para;</a></h3>
<ol>
<li>Reinstall proxmox on the host.</li>
</ol>
<p>Make sure to set the hostname and network the same as it was before.</p>
<ol>
<li>Update / Prepare the new OS</li>
</ol>
<p>At this point, I also ran the <a href="https://tteck.github.io/Proxmox/#proxmox-ve-post-install" target="_blank">tteck Post Install Script</a> to update the repos, disable enterprise repos, disable nag, etc.</p>
<p>And- I did a full <code>apt-get update &amp;&amp; apt-get dist-upgrade</code> to get everything installed, and updated.</p>
<p>Next- I logged into this machines GUI, directly by IP, and clicked on ceph, and told it to install the ceph packages for the version the rest of my cluster is using.</p>
<p>After the packages installed- I clocked out without completing the configuration.</p>
<p>Next, I ran my ansible-playbook to update the SSH keys, kernel power savings mode, apply IPMI scripts, install common packages and tools I use, and do basic machine configuration. These- steps don't impact proxmox.</p>
<p>At this point- you should have a fully functional stand-alone host. Now- we start the "fun" part.</p>
<h3 id="restoring-the-configuration">Restoring the configuration<a class="headerlink" href="#restoring-the-configuration" title="Permanent link">&para;</a></h3>
<div class="admonition danger">
<p class="admonition-title">Danger</p>
<p>These steps ARE completely unsupported, and may break your entire cluster!!!</p>
<p>I came up with these steps through my own process of trial and error.</p>
<p>This may not work for you. This may break your cluster and cause data loss.</p>
<p>Enter at your OWN risk. </p>
<p>I do not take any responsibility for any losses or damages which may occur from you following my unofficial 3rd party documentation.</p>
</div>
<p>With- the warning out of the way- I will say- I was able to completely recover my cluster.</p>
<p>This entire guide assumes you are running as root.</p>
<h4 id="step-1-prepare-remote-access-to-a-working-machine">Step 1. Prepare remote access to a working machine.<a class="headerlink" href="#step-1-prepare-remote-access-to-a-working-machine" title="Permanent link">&para;</a></h4>
<p>The first thing we need to do, is establish PKI between the new server, and one of the existing nodes.</p>
<p>Get the public key for your new server. <code>cat ~/.ssh/id_rsa.pub</code></p>
<p>If, for some reason, the above file does not exist, then create one. <code>ssh-keygen</code>, and then copy the public key from the above command.</p>
<p>Visit one of your working servers, as root, <code>nano ~/.ssh/authorized_keys</code>, and paste the public key in on a new line.</p>
<p>Go back to the new machine, we need to add the old machine to our hosts file, just to make this slightly faster/easier. Although, if you wish, you can do all of the rsync commands using the IP as well.</p>
<p>But- <code>nano /etc/hosts</code>, and add a line. <code>10.1.2.3    kube01</code> Save, and you are done.</p>
<p>After those two steps- this command should automatically establish a ssh session with the other server.</p>
<p><code>ssh root@oldmachine</code></p>
<div class="highlight"><pre><span></span><code>root@kube02:/var/lib/ceph#<span class="w"> </span>ssh<span class="w"> </span>root@kube01
Linux<span class="w"> </span>kube01<span class="w"> </span><span class="c1">#1 SMP PREEMPT_DYNAMIC PMX 6.8.4-2 (2024-04-10T17:36Z) x86_64</span>

The<span class="w"> </span>programs<span class="w"> </span>included<span class="w"> </span>with<span class="w"> </span>the<span class="w"> </span>Debian<span class="w"> </span>GNU/Linux<span class="w"> </span>system<span class="w"> </span>are<span class="w"> </span>free<span class="w"> </span>software<span class="p">;</span>
the<span class="w"> </span>exact<span class="w"> </span>distribution<span class="w"> </span>terms<span class="w"> </span><span class="k">for</span><span class="w"> </span>each<span class="w"> </span>program<span class="w"> </span>are<span class="w"> </span>described<span class="w"> </span><span class="k">in</span><span class="w"> </span>the
individual<span class="w"> </span>files<span class="w"> </span><span class="k">in</span><span class="w"> </span>/usr/share/doc/*/copyright.

Debian<span class="w"> </span>GNU/Linux<span class="w"> </span>comes<span class="w"> </span>with<span class="w"> </span>ABSOLUTELY<span class="w"> </span>NO<span class="w"> </span>WARRANTY,<span class="w"> </span>to<span class="w"> </span>the<span class="w"> </span>extent
permitted<span class="w"> </span>by<span class="w"> </span>applicable<span class="w"> </span>law.
Last<span class="w"> </span>login:<span class="w"> </span>Wed<span class="w"> </span>Jun<span class="w"> </span><span class="m">12</span><span class="w"> </span><span class="m">18</span>:35:22<span class="w"> </span><span class="m">2024</span><span class="w"> </span>from
root@kube01:~#
</code></pre></div>
<h4 id="step-2-completely-stop-proxmox">Step 2. COMPLETELY stop Proxmox<a class="headerlink" href="#step-2-completely-stop-proxmox" title="Permanent link">&para;</a></h4>
<p>Goto the new host, the one which we are installing. We need to stop ALL proxmox related services.</p>
<div class="highlight"><pre><span></span><code>systemctl<span class="w"> </span>stop<span class="w"> </span>pve-cluster
systemctl<span class="w"> </span>stop<span class="w"> </span>pvedaemon
systemctl<span class="w"> </span>stop<span class="w"> </span>pveproxy
systemctl<span class="w"> </span>stop<span class="w"> </span>pvestatd
systemctl<span class="w"> </span>stop<span class="w"> </span>corosync
systemctl<span class="w"> </span>stop<span class="w"> </span>pve-ha-lrm
systemctl<span class="w"> </span>stop<span class="w"> </span>pve-ha-crm
systemctl<span class="w"> </span>stop<span class="w"> </span>pve-firewall
systemctl<span class="w"> </span>stop<span class="w"> </span>pvescheduler
</code></pre></div>
<h4 id="step-3-prepare-etcpve">Step 3. Prepare /etc/pve<a class="headerlink" href="#step-3-prepare-etcpve" title="Permanent link">&para;</a></h4>
<p>The <code>/etc/pve</code> directly will automatically be synced from your other cluster nodes once corosync and pve-cluster are running.</p>
<p>We are going to move it elsewhere, and prepare the directory.</p>
<div class="highlight"><pre><span></span><code><span class="c1"># Move the pve directory somewhere else.</span>
mv<span class="w"> </span>/etc/pve<span class="w"> </span>~/pve_backup

<span class="c1"># Create a new PVE directory</span>
mkdir<span class="w"> </span>/etc/pve

chown<span class="w"> </span>root:root<span class="w"> </span>/etc/pve
</code></pre></div>
<h4 id="step-4-corosync">Step 4. Corosync<a class="headerlink" href="#step-4-corosync" title="Permanent link">&para;</a></h4>
<p>To get corosync running, we are just going to copy and paste from another host.</p>
<div class="highlight"><pre><span></span><code><span class="c1"># Update this to match the name of the source server, which is a cluster member that is currently running.</span>
<span class="nv">SOURCE</span><span class="o">=</span>your-old-server

<span class="c1"># Copy over corosync configuration</span>
scp<span class="w"> </span>root@<span class="nv">$SOURCE</span>:/etc/corosync/corosync.conf<span class="w"> </span>/etc/corosync/corosync.conf
scp<span class="w"> </span>root@<span class="nv">$SOURCE</span>:/etc/corosync/authkey<span class="w"> </span>/etc/corosync/authkey

<span class="c1"># Set permissions and ownership.</span>
chown<span class="w"> </span>root:root<span class="w"> </span>/etc/corosync/corosync.conf
chown<span class="w"> </span>root:root<span class="w"> </span>/etc/corosync/authkey
chmod<span class="w"> </span><span class="m">600</span><span class="w"> </span>/etc/corosync/authkey

<span class="c1"># Start corosync.</span>
systemctl<span class="w"> </span>start<span class="w"> </span>corosync

<span class="c1"># Watch journalctl output, to ensure its working. Once it looks happy, CTRL+C</span>
journalctl<span class="w"> </span>-fe
</code></pre></div>
<p>This, is all of the configuration we need for corosync to work. If, corosync fails, you will need to identify and correct the issue. <code>journalctl -fe</code> is a good place to start.</p>
<h4 id="step-5-pve-cluster">Step 5. PVE-Cluster<a class="headerlink" href="#step-5-pve-cluster" title="Permanent link">&para;</a></h4>
<p>At this point, you should have the corosync service running. We need to start on pve-cluster.</p>
<p>Start the service.</p>
<div class="highlight"><pre><span></span><code>systemctl<span class="w"> </span>start<span class="w"> </span>pve-cluster
</code></pre></div>
<p>But, you will notice- it throws an error when querying the status.</p>
<div class="highlight"><pre><span></span><code>root@kube02:/etc#<span class="w"> </span>pvecm<span class="w"> </span>status
Error:<span class="w"> </span>Corosync<span class="w"> </span>config<span class="w"> </span><span class="s1">&#39;/etc/pve/corosync.conf&#39;</span><span class="w"> </span>does<span class="w"> </span>not<span class="w"> </span>exist<span class="w"> </span>-<span class="w"> </span>is<span class="w"> </span>this<span class="w"> </span>node<span class="w"> </span>part<span class="w"> </span>of<span class="w"> </span>a<span class="w"> </span>cluster?
</code></pre></div>
<p>To fix this, we just need to copy the corosync file. Note- this step has to be done here. </p>
<div class="highlight"><pre><span></span><code>cp<span class="w"> </span>/etc/corosync/corosync.conf<span class="w"> </span>/etc/pve/
</code></pre></div>
<p>Here is my output from this process.
<div class="highlight"><pre><span></span><code>root@kube02:/etc#<span class="w"> </span>pvecm<span class="w"> </span>status
Error:<span class="w"> </span>Corosync<span class="w"> </span>config<span class="w"> </span><span class="s1">&#39;/etc/pve/corosync.conf&#39;</span><span class="w"> </span>does<span class="w"> </span>not<span class="w"> </span>exist<span class="w"> </span>-<span class="w"> </span>is<span class="w"> </span>this<span class="w"> </span>node<span class="w"> </span>part<span class="w"> </span>of<span class="w"> </span>a<span class="w"> </span>cluster?
root@kube02:/etc#<span class="w"> </span>cp<span class="w"> </span>/etc/corosync/corosync.conf<span class="w"> </span>/etc/pve/
root@kube02:/etc#<span class="w"> </span>pvecm<span class="w"> </span>status
Cluster<span class="w"> </span>information
-------------------
Name:<span class="w">             </span>Cluster0
Config<span class="w"> </span>Version:<span class="w">   </span><span class="m">11</span>
Transport:<span class="w">        </span>knet
Secure<span class="w"> </span>auth:<span class="w">      </span>on

Quorum<span class="w"> </span>information
------------------
Date:<span class="w">             </span>Wed<span class="w"> </span>Jun<span class="w"> </span><span class="m">12</span><span class="w"> </span><span class="m">18</span>:12:08<span class="w"> </span><span class="m">2024</span>
Quorum<span class="w"> </span>provider:<span class="w">  </span>corosync_votequorum
Nodes:<span class="w">            </span><span class="m">5</span>
Node<span class="w"> </span>ID:<span class="w">          </span>0x00000004
Ring<span class="w"> </span>ID:<span class="w">          </span><span class="m">1</span>.9e1
Quorate:<span class="w">          </span>Yes

Votequorum<span class="w"> </span>information
----------------------
Expected<span class="w"> </span>votes:<span class="w">   </span><span class="m">5</span>
Highest<span class="w"> </span>expected:<span class="w"> </span><span class="m">5</span>
Total<span class="w"> </span>votes:<span class="w">      </span><span class="m">5</span>
Quorum:<span class="w">           </span><span class="m">3</span>
Flags:<span class="w">            </span>Quorate

Membership<span class="w"> </span>information
----------------------
<span class="w">    </span>Nodeid<span class="w">      </span>Votes<span class="w"> </span>Name
0x00000001<span class="w">          </span><span class="m">1</span><span class="w"> </span><span class="m">10</span>.100.4.106
0x00000002<span class="w">          </span><span class="m">1</span><span class="w"> </span><span class="m">10</span>.100.4.100
0x00000003<span class="w">          </span><span class="m">1</span><span class="w"> </span><span class="m">10</span>.100.4.105
0x00000004<span class="w">          </span><span class="m">1</span><span class="w"> </span><span class="m">10</span>.100.4.102<span class="w"> </span><span class="o">(</span><span class="nb">local</span><span class="o">)</span>
0x00000005<span class="w">          </span><span class="m">1</span><span class="w"> </span><span class="m">10</span>.100.4.104
</code></pre></div></p>
<div class="admonition info">
<p class="admonition-title">Info</p>
<p>The corosync.conf needs to be copied to <code>/etc/pve</code> AFTER we start the pve-cluster service.</p>
</div>
<p>If you did this before starting pve-cluster, you would receive errors regarding the /etc/pve directly not being empty (as it mounts a cluster file system there.)</p>
<p>If- you did make that mistake- just start over at step 2.</p>
<h4 id="step-6-reboot">Step 6. Reboot<a class="headerlink" href="#step-6-reboot" title="Permanent link">&para;</a></h4>
<p>Reboot the host, and wait for it to come online.</p>
<p>After the host comes online- It should be acting like a cluster member now, and should be visible in your cluster's standard interface.</p>
<p>But- there a few more steps you may need to perform....</p>
<h4 id="step-7-reimport-zfs-pools">Step 7. Reimport ZFS Pools<a class="headerlink" href="#step-7-reimport-zfs-pools" title="Permanent link">&para;</a></h4>
<p>If, you have ZFS pools, you may need to re-import them.</p>
<p>You also may notice errors in your <code>journalctl -fe</code> output.</p>
<div class="highlight"><pre><span></span><code>Jun<span class="w"> </span><span class="m">12</span><span class="w"> </span><span class="m">18</span>:27:29<span class="w"> </span>kube02<span class="w"> </span>pvestatd<span class="o">[</span><span class="m">3033</span><span class="o">]</span>:<span class="w"> </span>storage<span class="w"> </span><span class="s1">&#39;NOYB&#39;</span><span class="w"> </span>is<span class="w"> </span>not<span class="w"> </span>online
Jun<span class="w"> </span><span class="m">12</span><span class="w"> </span><span class="m">18</span>:27:29<span class="w"> </span>kube02<span class="w"> </span>pvestatd<span class="o">[</span><span class="m">3033</span><span class="o">]</span>:<span class="w"> </span>zfs<span class="w"> </span>error:<span class="w"> </span>cannot<span class="w"> </span>open<span class="w"> </span><span class="s1">&#39;GameStorage&#39;</span>:<span class="w"> </span>no<span class="w"> </span>such<span class="w"> </span>pool
Jun<span class="w"> </span><span class="m">12</span><span class="w"> </span><span class="m">18</span>:27:30<span class="w"> </span>kube02<span class="w"> </span>pvestatd<span class="o">[</span><span class="m">3033</span><span class="o">]</span>:<span class="w"> </span>zfs<span class="w"> </span>error:<span class="w"> </span>cannot<span class="w"> </span>open<span class="w"> </span><span class="s1">&#39;GameStorage&#39;</span>:<span class="w"> </span>no<span class="w"> </span>such<span class="w"> </span>pool
Jun<span class="w"> </span><span class="m">12</span><span class="w"> </span><span class="m">18</span>:27:30<span class="w"> </span>kube02<span class="w"> </span>pvestatd<span class="o">[</span><span class="m">3033</span><span class="o">]</span>:<span class="w"> </span>could<span class="w"> </span>not<span class="w"> </span>activate<span class="w"> </span>storage<span class="w"> </span><span class="s1">&#39;GameStorage&#39;</span>,<span class="w"> </span>zfs<span class="w"> </span>error:<span class="w"> </span>The<span class="w"> </span>pool<span class="w"> </span>can<span class="w"> </span>be<span class="w"> </span>imported,<span class="w"> </span>use<span class="w"> </span><span class="s1">&#39;zpool import -f&#39;</span><span class="w"> </span>to<span class="w"> </span>import<span class="w"> </span>the<span class="w"> </span>pool.
Jun<span class="w"> </span><span class="m">12</span><span class="w"> </span><span class="m">18</span>:27:30<span class="w"> </span>kube02<span class="w"> </span>pvestatd<span class="o">[</span><span class="m">3033</span><span class="o">]</span>:<span class="w"> </span>storage<span class="w"> </span><span class="s1">&#39;ISOs&#39;</span><span class="w"> </span>is<span class="w"> </span>not<span class="w"> </span>online
Jun<span class="w"> </span><span class="m">12</span><span class="w"> </span><span class="m">18</span>:27:42<span class="w"> </span>kube02<span class="w"> </span>pvestatd<span class="o">[</span><span class="m">3033</span><span class="o">]</span>:<span class="w"> </span>storage<span class="w"> </span><span class="s1">&#39;NOYB&#39;</span><span class="w"> </span>is<span class="w"> </span>not<span class="w"> </span>online
Jun<span class="w"> </span><span class="m">12</span><span class="w"> </span><span class="m">18</span>:27:42<span class="w"> </span>kube02<span class="w"> </span>pvestatd<span class="o">[</span><span class="m">3033</span><span class="o">]</span>:<span class="w"> </span>zfs<span class="w"> </span>error:<span class="w"> </span>cannot<span class="w"> </span>open<span class="w"> </span><span class="s1">&#39;GameStorage&#39;</span>:<span class="w"> </span>no<span class="w"> </span>such<span class="w"> </span>pool
Jun<span class="w"> </span><span class="m">12</span><span class="w"> </span><span class="m">18</span>:27:42<span class="w"> </span>kube02<span class="w"> </span>pvestatd<span class="o">[</span><span class="m">3033</span><span class="o">]</span>:<span class="w"> </span>zfs<span class="w"> </span>error:<span class="w"> </span>cannot<span class="w"> </span>open<span class="w"> </span><span class="s1">&#39;GameStorage&#39;</span>:<span class="w"> </span>no<span class="w"> </span>such<span class="w"> </span>pool
Jun<span class="w"> </span><span class="m">12</span><span class="w"> </span><span class="m">18</span>:27:42<span class="w"> </span>kube02<span class="w"> </span>pvestatd<span class="o">[</span><span class="m">3033</span><span class="o">]</span>:<span class="w"> </span>could<span class="w"> </span>not<span class="w"> </span>activate<span class="w"> </span>storage<span class="w"> </span><span class="s1">&#39;GameStorage&#39;</span>,<span class="w"> </span>zfs<span class="w"> </span>error:<span class="w"> </span>The<span class="w"> </span>pool<span class="w"> </span>can<span class="w"> </span>be<span class="w"> </span>imported,<span class="w"> </span>use<span class="w"> </span><span class="s1">&#39;zpool import -f&#39;</span><span class="w"> </span>to<span class="w"> </span>import<span class="w"> </span>the<span class="w"> </span>pool.
</code></pre></div>
<p>The fix is simple enough, and is even provided in the log output: <code>zpool import YourPoolName -f</code></p>
<div class="highlight"><pre><span></span><code>root@kube02:/var/lib/ceph#<span class="w"> </span>zpool<span class="w"> </span>import<span class="w"> </span>GameStorage
cannot<span class="w"> </span>import<span class="w"> </span><span class="s1">&#39;GameStorage&#39;</span>:<span class="w"> </span>pool<span class="w"> </span>was<span class="w"> </span>previously<span class="w"> </span><span class="k">in</span><span class="w"> </span>use<span class="w"> </span>from<span class="w"> </span>another<span class="w"> </span>system.
Last<span class="w"> </span>accessed<span class="w"> </span>by<span class="w"> </span>kube02<span class="w"> </span><span class="o">(</span><span class="nv">hostid</span><span class="o">=</span>7060dad4<span class="o">)</span><span class="w"> </span>at<span class="w"> </span>Sun<span class="w"> </span>Jun<span class="w">  </span><span class="m">9</span><span class="w"> </span><span class="m">00</span>:24:41<span class="w"> </span><span class="m">2024</span>
The<span class="w"> </span>pool<span class="w"> </span>can<span class="w"> </span>be<span class="w"> </span>imported,<span class="w"> </span>use<span class="w"> </span><span class="s1">&#39;zpool import -f&#39;</span><span class="w"> </span>to<span class="w"> </span>import<span class="w"> </span>the<span class="w"> </span>pool.
root@kube02:/var/lib/ceph#<span class="w"> </span>zpool<span class="w"> </span>import<span class="w"> </span>GameStorage<span class="w"> </span>-f
root@kube02:/var/lib/ceph#<span class="w"> </span>zpool<span class="w"> </span>status
<span class="w">  </span>pool:<span class="w"> </span>GameStorage
<span class="w"> </span>state:<span class="w"> </span>ONLINE
<span class="w">  </span>scan:<span class="w"> </span>scrub<span class="w"> </span>repaired<span class="w"> </span>0B<span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="m">00</span>:00:40<span class="w"> </span>with<span class="w"> </span><span class="m">0</span><span class="w"> </span>errors<span class="w"> </span>on<span class="w"> </span>Sun<span class="w"> </span>Jun<span class="w">  </span><span class="m">9</span><span class="w"> </span><span class="m">00</span>:24:41<span class="w"> </span><span class="m">2024</span>
config:

<span class="w">        </span>NAME<span class="w">                                            </span>STATE<span class="w">     </span>READ<span class="w"> </span>WRITE<span class="w"> </span>CKSUM
<span class="w">        </span>GameStorage<span class="w">                                     </span>ONLINE<span class="w">       </span><span class="m">0</span><span class="w">     </span><span class="m">0</span><span class="w">     </span><span class="m">0</span>
<span class="w">          </span>nvme-Samsung_SSD_970_EVO_1TB_S5H9NS0NA99060M<span class="w">  </span>ONLINE<span class="w">       </span><span class="m">0</span><span class="w">     </span><span class="m">0</span><span class="w">     </span><span class="m">0</span>
</code></pre></div>
<p>At this point, you might also want to kick off a scrub. (My storage was very rudely shutdown when the boot drive died...)</p>
<p>To do so, just type <code>zpool scrub YourPoolName</code></p>
<p>It will continue running in the background with no additional attention needed.</p>
<div class="highlight"><pre><span></span><code>root@kube02:/var/lib/ceph#<span class="w"> </span>zpool<span class="w"> </span>status
<span class="w">  </span>pool:<span class="w"> </span>GameStorage
<span class="w"> </span>state:<span class="w"> </span>ONLINE
<span class="w">  </span>scan:<span class="w"> </span>scrub<span class="w"> </span><span class="k">in</span><span class="w"> </span>progress<span class="w"> </span>since<span class="w"> </span>Wed<span class="w"> </span>Jun<span class="w"> </span><span class="m">12</span><span class="w"> </span><span class="m">21</span>:02:55<span class="w"> </span><span class="m">2024</span>
<span class="w">        </span><span class="m">41</span>.7G<span class="w"> </span>/<span class="w"> </span><span class="m">41</span>.7G<span class="w"> </span>scanned,<span class="w"> </span><span class="m">6</span>.44G<span class="w"> </span>/<span class="w"> </span><span class="m">41</span>.7G<span class="w"> </span>issued<span class="w"> </span>at<span class="w"> </span>549M/s
<span class="w">        </span>0B<span class="w"> </span>repaired,<span class="w"> </span><span class="m">15</span>.44%<span class="w"> </span><span class="k">done</span>,<span class="w"> </span><span class="m">00</span>:01:05<span class="w"> </span>to<span class="w"> </span>go
config:

<span class="w">        </span>NAME<span class="w">                                            </span>STATE<span class="w">     </span>READ<span class="w"> </span>WRITE<span class="w"> </span>CKSUM
<span class="w">        </span>GameStorage<span class="w">                                     </span>ONLINE<span class="w">       </span><span class="m">0</span><span class="w">     </span><span class="m">0</span><span class="w">     </span><span class="m">0</span>
<span class="w">          </span>nvme-Samsung_SSD_970_EVO_1TB_S5H9NS0NA99060M<span class="w">  </span>ONLINE<span class="w">       </span><span class="m">0</span><span class="w">     </span><span class="m">0</span><span class="w">     </span><span class="m">0</span>

<span class="c1">#And, eventually, it will finish</span>
root@kube02:/var/lib/ceph#<span class="w"> </span>zpool<span class="w"> </span>status
<span class="w">  </span>pool:<span class="w"> </span>GameStorage
<span class="w"> </span>state:<span class="w"> </span>ONLINE
<span class="w">  </span>scan:<span class="w"> </span>scrub<span class="w"> </span>repaired<span class="w"> </span>0B<span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="m">00</span>:01:37<span class="w"> </span>with<span class="w"> </span><span class="m">0</span><span class="w"> </span>errors<span class="w"> </span>on<span class="w"> </span>Wed<span class="w"> </span>Jun<span class="w"> </span><span class="m">12</span><span class="w"> </span><span class="m">21</span>:04:32<span class="w"> </span><span class="m">2024</span>
config:

<span class="w">        </span>NAME<span class="w">                                            </span>STATE<span class="w">     </span>READ<span class="w"> </span>WRITE<span class="w"> </span>CKSUM
<span class="w">        </span>GameStorage<span class="w">                                     </span>ONLINE<span class="w">       </span><span class="m">0</span><span class="w">     </span><span class="m">0</span><span class="w">     </span><span class="m">0</span>
<span class="w">          </span>nvme-Samsung_SSD_970_EVO_1TB_S5H9NS0NA99060M<span class="w">  </span>ONLINE<span class="w">       </span><span class="m">0</span><span class="w">     </span><span class="m">0</span><span class="w">     </span><span class="m">0</span>
</code></pre></div>
<h4 id="step-8-reimport-ceph-osds">Step 8. Reimport Ceph OSDs<a class="headerlink" href="#step-8-reimport-ceph-osds" title="Permanent link">&para;</a></h4>
<div class="admonition info">
<p class="admonition-title">Info</p>
<p>This step assumes, you have a "somewhat" healthy ceph cluster at this point, and are just needing to reimport your existing OSDs.</p>
<p>If, you don't have functional monitors, and managers, you need to correct that first.</p>
</div>
<p>Since, I am running ceph, my OSDs are currently not running. We need to correct this.</p>
<div class="admonition info">
<p class="admonition-title">Info</p>
<p>This step can be used anytime you move a ceph OSD from one host to another, without rebuilding the OSD.</p>
</div>
<p>So- the first step- just make sure your ceph volumes are still there. They shouldn't be mounted at this point, as we are on a pretty fresh install still.</p>
<h5 id="gathering-information">Gathering information<a class="headerlink" href="#gathering-information" title="Permanent link">&para;</a></h5>
<p>You, don't really NEED any information from this step- It just details on how to find and view the information.</p>
<p>If, you are sure your OSDs are on the host, and just aren't running, skip to the next step.</p>
<div class="highlight"><pre><span></span><code>root@kube02:/var/lib/ceph#<span class="w"> </span>lsblk
NAME<span class="w">                                                                                                  </span>MAJ:MIN<span class="w"> </span>RM<span class="w">   </span>SIZE<span class="w"> </span>RO<span class="w"> </span>TYPE<span class="w"> </span>MOUNTPOINTS
sda<span class="w">                                                                                                     </span><span class="m">8</span>:0<span class="w">    </span><span class="m">0</span><span class="w">  </span><span class="m">14</span>.6T<span class="w">  </span><span class="m">0</span><span class="w"> </span>disk
sda1<span class="w">                                                                                                  </span><span class="m">8</span>:1<span class="w">    </span><span class="m">0</span><span class="w">  </span><span class="m">14</span>.6T<span class="w">  </span><span class="m">0</span><span class="w"> </span>part
sdb<span class="w">                                                                                                     </span><span class="m">8</span>:16<span class="w">   </span><span class="m">0</span><span class="w">   </span><span class="m">7</span>.3T<span class="w">  </span><span class="m">0</span><span class="w"> </span>disk
sdb1<span class="w">                                                                                                  </span><span class="m">8</span>:17<span class="w">   </span><span class="m">0</span><span class="w">   </span><span class="m">7</span>.3T<span class="w">  </span><span class="m">0</span><span class="w"> </span>part
sdc<span class="w">                                                                                                     </span><span class="m">8</span>:32<span class="w">   </span><span class="m">0</span><span class="w">   </span><span class="m">7</span>.3T<span class="w">  </span><span class="m">0</span><span class="w"> </span>disk
sdc1<span class="w">                                                                                                  </span><span class="m">8</span>:33<span class="w">   </span><span class="m">0</span><span class="w">   </span><span class="m">7</span>.3T<span class="w">  </span><span class="m">0</span><span class="w"> </span>part
sdd<span class="w">                                                                                                     </span><span class="m">8</span>:48<span class="w">   </span><span class="m">0</span><span class="w">  </span><span class="m">14</span>.6T<span class="w">  </span><span class="m">0</span><span class="w"> </span>disk
sdd1<span class="w">                                                                                                  </span><span class="m">8</span>:49<span class="w">   </span><span class="m">0</span><span class="w">  </span><span class="m">14</span>.6T<span class="w">  </span><span class="m">0</span><span class="w"> </span>part
sde<span class="w">                                                                                                     </span><span class="m">8</span>:64<span class="w">   </span><span class="m">0</span><span class="w">   </span><span class="m">7</span>.3T<span class="w">  </span><span class="m">0</span><span class="w"> </span>disk
sde1<span class="w">                                                                                                  </span><span class="m">8</span>:65<span class="w">   </span><span class="m">0</span><span class="w">   </span><span class="m">7</span>.3T<span class="w">  </span><span class="m">0</span><span class="w"> </span>part
sdf<span class="w">                                                                                                     </span><span class="m">8</span>:80<span class="w">   </span><span class="m">0</span><span class="w">   </span><span class="m">7</span>.3T<span class="w">  </span><span class="m">0</span><span class="w"> </span>disk
sdf1<span class="w">                                                                                                  </span><span class="m">8</span>:81<span class="w">   </span><span class="m">0</span><span class="w">   </span><span class="m">7</span>.3T<span class="w">  </span><span class="m">0</span><span class="w"> </span>part
sdg<span class="w">                                                                                                     </span><span class="m">8</span>:96<span class="w">   </span><span class="m">0</span><span class="w">  </span><span class="m">14</span>.6T<span class="w">  </span><span class="m">0</span><span class="w"> </span>disk
sdg1<span class="w">                                                                                                  </span><span class="m">8</span>:97<span class="w">   </span><span class="m">0</span><span class="w">  </span><span class="m">14</span>.6T<span class="w">  </span><span class="m">0</span><span class="w"> </span>part
sdh<span class="w">                                                                                                     </span><span class="m">8</span>:112<span class="w">  </span><span class="m">0</span><span class="w">   </span><span class="m">7</span>.3T<span class="w">  </span><span class="m">0</span><span class="w"> </span>disk
sdh1<span class="w">                                                                                                  </span><span class="m">8</span>:113<span class="w">  </span><span class="m">0</span><span class="w">   </span><span class="m">7</span>.3T<span class="w">  </span><span class="m">0</span><span class="w"> </span>part
sdi<span class="w">                                                                                                     </span><span class="m">8</span>:128<span class="w">  </span><span class="m">0</span><span class="w">  </span><span class="m">14</span>.6T<span class="w">  </span><span class="m">0</span><span class="w"> </span>disk
sdi1<span class="w">                                                                                                  </span><span class="m">8</span>:129<span class="w">  </span><span class="m">0</span><span class="w">  </span><span class="m">14</span>.6T<span class="w">  </span><span class="m">0</span><span class="w"> </span>part
sdj<span class="w">                                                                                                     </span><span class="m">8</span>:144<span class="w">  </span><span class="m">1</span><span class="w">   </span><span class="m">7</span>.5G<span class="w">  </span><span class="m">0</span><span class="w"> </span>disk
sdj1<span class="w">                                                                                                  </span><span class="m">8</span>:145<span class="w">  </span><span class="m">1</span><span class="w">   </span><span class="m">7</span>.5G<span class="w">  </span><span class="m">0</span><span class="w"> </span>part
sdk<span class="w">                                                                                                     </span><span class="m">8</span>:160<span class="w">  </span><span class="m">1</span><span class="w">  </span><span class="m">29</span>.9G<span class="w">  </span><span class="m">0</span><span class="w"> </span>disk
sdk1<span class="w">                                                                                                  </span><span class="m">8</span>:161<span class="w">  </span><span class="m">1</span><span class="w">  </span><span class="m">29</span>.9G<span class="w">  </span><span class="m">0</span><span class="w"> </span>part
nvme0n1<span class="w">                                                                                               </span><span class="m">259</span>:0<span class="w">    </span><span class="m">0</span><span class="w"> </span><span class="m">894</span>.3G<span class="w">  </span><span class="m">0</span><span class="w"> </span>disk
nvme0n1p1<span class="w">                                                                                           </span><span class="m">259</span>:1<span class="w">    </span><span class="m">0</span><span class="w">  </span>1007K<span class="w">  </span><span class="m">0</span><span class="w"> </span>part
nvme0n1p2<span class="w">                                                                                           </span><span class="m">259</span>:2<span class="w">    </span><span class="m">0</span><span class="w">     </span>1G<span class="w">  </span><span class="m">0</span><span class="w"> </span>part<span class="w"> </span>/boot/efi
nvme0n1p3<span class="w">                                                                                           </span><span class="m">259</span>:3<span class="w">    </span><span class="m">0</span><span class="w"> </span><span class="m">893</span>.3G<span class="w">  </span><span class="m">0</span><span class="w"> </span>part
<span class="w">  </span>pve-swap<span class="w">                                                                                          </span><span class="m">252</span>:3<span class="w">    </span><span class="m">0</span><span class="w">     </span>8G<span class="w">  </span><span class="m">0</span><span class="w"> </span>lvm<span class="w">  </span><span class="o">[</span>SWAP<span class="o">]</span>
<span class="w">  </span>pve-root<span class="w">                                                                                          </span><span class="m">252</span>:4<span class="w">    </span><span class="m">0</span><span class="w">    </span>96G<span class="w">  </span><span class="m">0</span><span class="w"> </span>lvm<span class="w">  </span>/
<span class="w">  </span>pve-data_tmeta<span class="w">                                                                                    </span><span class="m">252</span>:6<span class="w">    </span><span class="m">0</span><span class="w">   </span><span class="m">7</span>.7G<span class="w">  </span><span class="m">0</span><span class="w"> </span>lvm
<span class="w">  </span><span class="w"> </span>pve-data<span class="w">                                                                                        </span><span class="m">252</span>:8<span class="w">    </span><span class="m">0</span><span class="w"> </span><span class="m">757</span>.8G<span class="w">  </span><span class="m">0</span><span class="w"> </span>lvm
<span class="w">  </span>pve-data_tdata<span class="w">                                                                                    </span><span class="m">252</span>:7<span class="w">    </span><span class="m">0</span><span class="w"> </span><span class="m">757</span>.8G<span class="w">  </span><span class="m">0</span><span class="w"> </span>lvm
<span class="w">    </span>pve-data<span class="w">                                                                                        </span><span class="m">252</span>:8<span class="w">    </span><span class="m">0</span><span class="w"> </span><span class="m">757</span>.8G<span class="w">  </span><span class="m">0</span><span class="w"> </span>lvm
nvme6n1<span class="w">                                                                                               </span><span class="m">259</span>:4<span class="w">    </span><span class="m">0</span><span class="w"> </span><span class="m">894</span>.3G<span class="w">  </span><span class="m">0</span><span class="w"> </span>disk
ceph--126d8000--5cdd--4ced--ab77--38893cd88dc8-osd--block--6db92940--fc38--4b0b--9fb4--4c5ac6742921<span class="w"> </span><span class="m">252</span>:5<span class="w">    </span><span class="m">0</span><span class="w"> </span><span class="m">894</span>.3G<span class="w">  </span><span class="m">0</span><span class="w"> </span>lvm
nvme5n1<span class="w">                                                                                               </span><span class="m">259</span>:5<span class="w">    </span><span class="m">0</span><span class="w"> </span><span class="m">894</span>.3G<span class="w">  </span><span class="m">0</span><span class="w"> </span>disk
ceph--8e2310de--b5ed--46bd--a32e--5048af46cfe0-osd--block--e06ae6d2--d881--40a0--adcd--2c55020474d5<span class="w"> </span><span class="m">252</span>:2<span class="w">    </span><span class="m">0</span><span class="w"> </span><span class="m">894</span>.3G<span class="w">  </span><span class="m">0</span><span class="w"> </span>lvm
nvme7n1<span class="w">                                                                                               </span><span class="m">259</span>:6<span class="w">    </span><span class="m">0</span><span class="w"> </span><span class="m">894</span>.3G<span class="w">  </span><span class="m">0</span><span class="w"> </span>disk
ceph--3f428911--5213--461d--a5fd--029f673d7ad2-osd--block--e65f3c81--23de--4f81--af2b--be185f6fef42<span class="w"> </span><span class="m">252</span>:1<span class="w">    </span><span class="m">0</span><span class="w"> </span><span class="m">894</span>.3G<span class="w">  </span><span class="m">0</span><span class="w"> </span>lvm
nvme8n1<span class="w">                                                                                               </span><span class="m">259</span>:7<span class="w">    </span><span class="m">0</span><span class="w"> </span><span class="m">894</span>.3G<span class="w">  </span><span class="m">0</span><span class="w"> </span>disk
ceph--25d71cae--e648--4a95--ae1f--c45f8fa547d7-osd--block--ae41fc24--8408--4656--8310--58e00a2a146b<span class="w"> </span><span class="m">252</span>:0<span class="w">    </span><span class="m">0</span><span class="w"> </span><span class="m">894</span>.3G<span class="w">  </span><span class="m">0</span><span class="w"> </span>lvm
nvme2n1<span class="w">                                                                                               </span><span class="m">259</span>:8<span class="w">    </span><span class="m">0</span><span class="w"> </span><span class="m">931</span>.5G<span class="w">  </span><span class="m">0</span><span class="w"> </span>disk
nvme2n1p1<span class="w">                                                                                           </span><span class="m">259</span>:9<span class="w">    </span><span class="m">0</span><span class="w"> </span><span class="m">931</span>.5G<span class="w">  </span><span class="m">0</span><span class="w"> </span>part
nvme2n1p9<span class="w">                                                                                           </span><span class="m">259</span>:10<span class="w">   </span><span class="m">0</span><span class="w">     </span>8M<span class="w">  </span><span class="m">0</span><span class="w"> </span>part
nvme1n1<span class="w">                                                                                               </span><span class="m">259</span>:11<span class="w">   </span><span class="m">0</span><span class="w"> </span><span class="m">931</span>.5G<span class="w">  </span><span class="m">0</span><span class="w"> </span>disk
nvme1n1p1<span class="w">                                                                                           </span><span class="m">259</span>:12<span class="w">   </span><span class="m">0</span><span class="w"> </span><span class="m">931</span>.5G<span class="w">  </span><span class="m">0</span><span class="w"> </span>part
nvme4n1<span class="w">                                                                                               </span><span class="m">259</span>:13<span class="w">   </span><span class="m">0</span><span class="w"> </span><span class="m">931</span>.5G<span class="w">  </span><span class="m">0</span><span class="w"> </span>disk
nvme4n1p1<span class="w">                                                                                           </span><span class="m">259</span>:15<span class="w">   </span><span class="m">0</span><span class="w"> </span><span class="m">931</span>.5G<span class="w">  </span><span class="m">0</span><span class="w"> </span>part
nvme3n1<span class="w">                                                                                               </span><span class="m">259</span>:14<span class="w">   </span><span class="m">0</span><span class="w"> </span><span class="m">931</span>.5G<span class="w">  </span><span class="m">0</span><span class="w"> </span>disk
nvme3n1p1<span class="w">  </span>
</code></pre></div>
<p>And- as we can see- they are here, nvme6 through nvme8.</p>
<p>A few commands to gather and show information-</p>
<p>In the status, we can see multiple OSDs are down, in this part: <code>1 host (4 osds) down</code></p>
<div class="highlight"><pre><span></span><code>root@kube02:/var/lib/ceph#<span class="w"> </span>ceph<span class="w"> </span>status
<span class="w">  </span>cluster:
<span class="w">    </span>id:<span class="w">     </span>016d27bb-5e11-4c85-846a-98f1f0b7ec08
<span class="w">    </span>health:<span class="w"> </span>HEALTH_WARN
<span class="w">            </span><span class="m">1</span><span class="w"> </span>osds<span class="w"> </span>down
<span class="w">            </span><span class="m">1</span><span class="w"> </span>host<span class="w"> </span><span class="o">(</span><span class="m">4</span><span class="w"> </span>osds<span class="o">)</span><span class="w"> </span>down
<span class="w">            </span>Degraded<span class="w"> </span>data<span class="w"> </span>redundancy:<span class="w"> </span><span class="m">593570</span>/1780710<span class="w"> </span>objects<span class="w"> </span>degraded<span class="w"> </span><span class="o">(</span><span class="m">33</span>.333%<span class="o">)</span>,<span class="w"> </span><span class="m">78</span><span class="w"> </span>pgs<span class="w"> </span>degraded,<span class="w"> </span><span class="m">129</span><span class="w"> </span>pgs<span class="w"> </span>undersized

<span class="w">  </span>services:
<span class="w">    </span>mon:<span class="w"> </span><span class="m">2</span><span class="w"> </span>daemons,<span class="w"> </span>quorum<span class="w"> </span>kube05,kube01<span class="w"> </span><span class="o">(</span>age<span class="w"> </span>16m<span class="o">)</span>
<span class="w">    </span>mgr:<span class="w"> </span>kube05<span class="o">(</span>active,<span class="w"> </span>since<span class="w"> </span>2h<span class="o">)</span>,<span class="w"> </span>standbys:<span class="w"> </span>kube01
<span class="w">    </span>mds:<span class="w"> </span><span class="m">1</span>/1<span class="w"> </span>daemons<span class="w"> </span>up,<span class="w"> </span><span class="m">1</span><span class="w"> </span>standby
<span class="w">    </span>osd:<span class="w"> </span><span class="m">13</span><span class="w"> </span>osds:<span class="w"> </span><span class="m">9</span><span class="w"> </span>up<span class="w"> </span><span class="o">(</span>since<span class="w"> </span>15m<span class="o">)</span>,<span class="w"> </span><span class="m">10</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="o">(</span>since<span class="w"> </span>3m<span class="o">)</span>

<span class="w">  </span>data:
<span class="w">    </span>volumes:<span class="w"> </span><span class="m">1</span>/1<span class="w"> </span>healthy
<span class="w">    </span>pools:<span class="w">   </span><span class="m">5</span><span class="w"> </span>pools,<span class="w"> </span><span class="m">129</span><span class="w"> </span>pgs
<span class="w">    </span>objects:<span class="w"> </span><span class="m">593</span>.57k<span class="w"> </span>objects,<span class="w"> </span><span class="m">1</span>.3<span class="w"> </span>TiB
<span class="w">    </span>usage:<span class="w">   </span><span class="m">2</span>.5<span class="w"> </span>TiB<span class="w"> </span>used,<span class="w"> </span><span class="m">8</span>.8<span class="w"> </span>TiB<span class="w"> </span>/<span class="w"> </span><span class="m">11</span><span class="w"> </span>TiB<span class="w"> </span>avail
<span class="w">    </span>pgs:<span class="w">     </span><span class="m">593570</span>/1780710<span class="w"> </span>objects<span class="w"> </span>degraded<span class="w"> </span><span class="o">(</span><span class="m">33</span>.333%<span class="o">)</span>
<span class="w">             </span><span class="m">78</span><span class="w"> </span>active+undersized+degraded
<span class="w">             </span><span class="m">51</span><span class="w"> </span>active+undersized

<span class="w">  </span>io:
<span class="w">    </span>client:<span class="w">   </span><span class="m">3</span>.5<span class="w"> </span>MiB/s<span class="w"> </span>rd,<span class="w"> </span><span class="m">1</span>.6<span class="w"> </span>MiB/s<span class="w"> </span>wr,<span class="w"> </span><span class="m">540</span><span class="w"> </span>op/s<span class="w"> </span>rd,<span class="w"> </span><span class="m">192</span><span class="w"> </span>op/s<span class="w"> </span>wr
</code></pre></div>
<p>Next- we check the lvms. You should see all of your missing OSDs here.</p>
<div class="highlight"><pre><span></span><code>root@kube02:/var/lib/ceph#<span class="w"> </span>ceph-volume<span class="w"> </span>lvm<span class="w"> </span><span class="nv">list</span>


<span class="o">======</span><span class="w"> </span>osd.3<span class="w"> </span><span class="o">=======</span>

<span class="w">  </span><span class="o">[</span>block<span class="o">]</span><span class="w">       </span>/dev/ceph-3f428911-5213-461d-a5fd-029f673d7ad2/osd-block-e65f3c81-23de-4f81-af2b-be185f6fef42

<span class="w">      </span>block<span class="w"> </span>device<span class="w">              </span>/dev/ceph-3f428911-5213-461d-a5fd-029f673d7ad2/osd-block-e65f3c81-23de-4f81-af2b-be185f6fef42
<span class="w">      </span>block<span class="w"> </span>uuid<span class="w">                </span>qYPoy2-fQ4B-GeCy-yO4J-9KgO-tVfb-KT3Qeq
<span class="w">      </span>cephx<span class="w"> </span>lockbox<span class="w"> </span>secret
<span class="w">      </span>cluster<span class="w"> </span>fsid<span class="w">              </span>016d27bb-5e11-4c85-846a-98f1f0b7ec08
<span class="w">      </span>cluster<span class="w"> </span>name<span class="w">              </span>ceph
<span class="w">      </span>crush<span class="w"> </span>device<span class="w"> </span>class
<span class="w">      </span>encrypted<span class="w">                 </span><span class="m">0</span>
<span class="w">      </span>osd<span class="w"> </span>fsid<span class="w">                  </span>e65f3c81-23de-4f81-af2b-be185f6fef42
<span class="w">      </span>osd<span class="w"> </span>id<span class="w">                    </span><span class="m">3</span>
<span class="w">      </span>osdspec<span class="w"> </span>affinity
<span class="w">      </span><span class="nb">type</span><span class="w">                      </span>block
<span class="w">      </span>vdo<span class="w">                       </span><span class="m">0</span>
<span class="w">      </span>devices<span class="w">                   </span>/dev/nvme7n1

<span class="o">======</span><span class="w"> </span>osd.5<span class="w"> </span><span class="o">=======</span>

<span class="w">  </span><span class="o">[</span>block<span class="o">]</span><span class="w">       </span>/dev/ceph-8e2310de-b5ed-46bd-a32e-5048af46cfe0/osd-block-e06ae6d2-d881-40a0-adcd-2c55020474d5

<span class="w">      </span>block<span class="w"> </span>device<span class="w">              </span>/dev/ceph-8e2310de-b5ed-46bd-a32e-5048af46cfe0/osd-block-e06ae6d2-d881-40a0-adcd-2c55020474d5
<span class="w">      </span>block<span class="w"> </span>uuid<span class="w">                </span>20e763-MU1H-9Yem-Qtru-R99T-tULs-V0QZhf
<span class="w">      </span>cephx<span class="w"> </span>lockbox<span class="w"> </span>secret
<span class="w">      </span>cluster<span class="w"> </span>fsid<span class="w">              </span>016d27bb-5e11-4c85-846a-98f1f0b7ec08
<span class="w">      </span>cluster<span class="w"> </span>name<span class="w">              </span>ceph
<span class="w">      </span>crush<span class="w"> </span>device<span class="w"> </span>class
<span class="w">      </span>encrypted<span class="w">                 </span><span class="m">0</span>
<span class="w">      </span>osd<span class="w"> </span>fsid<span class="w">                  </span>e06ae6d2-d881-40a0-adcd-2c55020474d5
<span class="w">      </span>osd<span class="w"> </span>id<span class="w">                    </span><span class="m">5</span>
<span class="w">      </span>osdspec<span class="w"> </span>affinity
<span class="w">      </span><span class="nb">type</span><span class="w">                      </span>block
<span class="w">      </span>vdo<span class="w">                       </span><span class="m">0</span>
<span class="w">      </span>devices<span class="w">                   </span>/dev/nvme5n1

<span class="o">======</span><span class="w"> </span>osd.6<span class="w"> </span><span class="o">=======</span>

<span class="w">  </span><span class="o">[</span>block<span class="o">]</span><span class="w">       </span>/dev/ceph-126d8000-5cdd-4ced-ab77-38893cd88dc8/osd-block-6db92940-fc38-4b0b-9fb4-4c5ac6742921

<span class="w">      </span>block<span class="w"> </span>device<span class="w">              </span>/dev/ceph-126d8000-5cdd-4ced-ab77-38893cd88dc8/osd-block-6db92940-fc38-4b0b-9fb4-4c5ac6742921
<span class="w">      </span>block<span class="w"> </span>uuid<span class="w">                </span>qLe3KP-d5we-EENg-SlkG-KDuc-Ri3f-xivchq
<span class="w">      </span>cephx<span class="w"> </span>lockbox<span class="w"> </span>secret
<span class="w">      </span>cluster<span class="w"> </span>fsid<span class="w">              </span>016d27bb-5e11-4c85-846a-98f1f0b7ec08
<span class="w">      </span>cluster<span class="w"> </span>name<span class="w">              </span>ceph
<span class="w">      </span>crush<span class="w"> </span>device<span class="w"> </span>class
<span class="w">      </span>encrypted<span class="w">                 </span><span class="m">0</span>
<span class="w">      </span>osd<span class="w"> </span>fsid<span class="w">                  </span>6db92940-fc38-4b0b-9fb4-4c5ac6742921
<span class="w">      </span>osd<span class="w"> </span>id<span class="w">                    </span><span class="m">6</span>
<span class="w">      </span>osdspec<span class="w"> </span>affinity
<span class="w">      </span><span class="nb">type</span><span class="w">                      </span>block
<span class="w">      </span>vdo<span class="w">                       </span><span class="m">0</span>
<span class="w">      </span>devices<span class="w">                   </span>/dev/nvme6n1

<span class="o">======</span><span class="w"> </span>osd.7<span class="w"> </span><span class="o">=======</span>

<span class="w">  </span><span class="o">[</span>block<span class="o">]</span><span class="w">       </span>/dev/ceph-25d71cae-e648-4a95-ae1f-c45f8fa547d7/osd-block-ae41fc24-8408-4656-8310-58e00a2a146b

<span class="w">      </span>block<span class="w"> </span>device<span class="w">              </span>/dev/ceph-25d71cae-e648-4a95-ae1f-c45f8fa547d7/osd-block-ae41fc24-8408-4656-8310-58e00a2a146b
<span class="w">      </span>block<span class="w"> </span>uuid<span class="w">                </span>bodGoC-XlhL-1TxK-x6K9-QqnF-h3LT-TRYkHy
<span class="w">      </span>cephx<span class="w"> </span>lockbox<span class="w"> </span>secret
<span class="w">      </span>cluster<span class="w"> </span>fsid<span class="w">              </span>016d27bb-5e11-4c85-846a-98f1f0b7ec08
<span class="w">      </span>cluster<span class="w"> </span>name<span class="w">              </span>ceph
<span class="w">      </span>crush<span class="w"> </span>device<span class="w"> </span>class
<span class="w">      </span>encrypted<span class="w">                 </span><span class="m">0</span>
<span class="w">      </span>osd<span class="w"> </span>fsid<span class="w">                  </span>ae41fc24-8408-4656-8310-58e00a2a146b
<span class="w">      </span>osd<span class="w"> </span>id<span class="w">                    </span><span class="m">7</span>
<span class="w">      </span>osdspec<span class="w"> </span>affinity
<span class="w">      </span><span class="nb">type</span><span class="w">                      </span>block
<span class="w">      </span>vdo<span class="w">                       </span><span class="m">0</span>
<span class="w">      </span>devices<span class="w">                   </span>/dev/nvme8n1
</code></pre></div>
<h5 id="re-import-the-lvms">Re-import the LVMs!<a class="headerlink" href="#re-import-the-lvms" title="Permanent link">&para;</a></h5>
<p>So- you might assume you need to manually rebuild systemd units, update configurations for proxmox, ceph, and do funky ceph commands.... But- that is not the case.</p>
<p>Just enter this command: <code>ceph-volume lvm activate --all</code></p>
<p>Thats it!</p>
<p>Ceph will automatically re-mount the LVMs, create links, set ownership, create systemd unit files, and... basically handle nearly everything for you.</p>
<div class="highlight"><pre><span></span><code>root@kube02:/var/lib/ceph#<span class="w"> </span>ceph-volume<span class="w"> </span>lvm<span class="w"> </span>activate<span class="w"> </span>--all
--&gt;<span class="w"> </span>Activating<span class="w"> </span>OSD<span class="w"> </span>ID<span class="w"> </span><span class="m">6</span><span class="w"> </span>FSID<span class="w"> </span>6db92940-fc38-4b0b-9fb4-4c5ac6742921
Running<span class="w"> </span>command:<span class="w"> </span>/usr/bin/mount<span class="w"> </span>-t<span class="w"> </span>tmpfs<span class="w"> </span>tmpfs<span class="w"> </span>/var/lib/ceph/osd/ceph-6
--&gt;<span class="w"> </span>Executable<span class="w"> </span>selinuxenabled<span class="w"> </span>not<span class="w"> </span><span class="k">in</span><span class="w"> </span>PATH:<span class="w"> </span>/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
Running<span class="w"> </span>command:<span class="w"> </span>/usr/bin/chown<span class="w"> </span>-R<span class="w"> </span>ceph:ceph<span class="w"> </span>/var/lib/ceph/osd/ceph-6
Running<span class="w"> </span>command:<span class="w"> </span>/usr/bin/ceph-bluestore-tool<span class="w"> </span>--cluster<span class="o">=</span>ceph<span class="w"> </span>prime-osd-dir<span class="w"> </span>--dev<span class="w"> </span>/dev/ceph-126d8000-5cdd-4ced-ab77-38893cd88dc8/osd-block-6db92940-fc38-4b0b-9fb4-4c5ac6742921<span class="w"> </span>--path<span class="w"> </span>/var/lib/ceph/osd/ceph-6<span class="w"> </span>--no-mon-config
Running<span class="w"> </span>command:<span class="w"> </span>/usr/bin/ln<span class="w"> </span>-snf<span class="w"> </span>/dev/ceph-126d8000-5cdd-4ced-ab77-38893cd88dc8/osd-block-6db92940-fc38-4b0b-9fb4-4c5ac6742921<span class="w"> </span>/var/lib/ceph/osd/ceph-6/block
Running<span class="w"> </span>command:<span class="w"> </span>/usr/bin/chown<span class="w"> </span>-h<span class="w"> </span>ceph:ceph<span class="w"> </span>/var/lib/ceph/osd/ceph-6/block
Running<span class="w"> </span>command:<span class="w"> </span>/usr/bin/chown<span class="w"> </span>-R<span class="w"> </span>ceph:ceph<span class="w"> </span>/dev/dm-5
Running<span class="w"> </span>command:<span class="w"> </span>/usr/bin/chown<span class="w"> </span>-R<span class="w"> </span>ceph:ceph<span class="w"> </span>/var/lib/ceph/osd/ceph-6
Running<span class="w"> </span>command:<span class="w"> </span>/usr/bin/systemctl<span class="w"> </span><span class="nb">enable</span><span class="w"> </span>ceph-volume@lvm-6-6db92940-fc38-4b0b-9fb4-4c5ac6742921
<span class="w"> </span>stderr:<span class="w"> </span>Created<span class="w"> </span>symlink<span class="w"> </span>/etc/systemd/system/multi-user.target.wants/ceph-volume@lvm-6-6db92940-fc38-4b0b-9fb4-4c5ac6742921.service<span class="w"> </span><span class="w"> </span>/lib/systemd/system/ceph-volume@.service.
Running<span class="w"> </span>command:<span class="w"> </span>/usr/bin/systemctl<span class="w"> </span><span class="nb">enable</span><span class="w"> </span>--runtime<span class="w"> </span>ceph-osd@6
<span class="w"> </span>stderr:<span class="w"> </span>Created<span class="w"> </span>symlink<span class="w"> </span>/run/systemd/system/ceph-osd.target.wants/ceph-osd@6.service<span class="w"> </span><span class="w"> </span>/lib/systemd/system/ceph-osd@.service.
Running<span class="w"> </span>command:<span class="w"> </span>/usr/bin/systemctl<span class="w"> </span>start<span class="w"> </span>ceph-osd@6
--&gt;<span class="w"> </span>ceph-volume<span class="w"> </span>lvm<span class="w"> </span>activate<span class="w"> </span>successful<span class="w"> </span><span class="k">for</span><span class="w"> </span>osd<span class="w"> </span>ID:<span class="w"> </span><span class="m">6</span>
--&gt;<span class="w"> </span>Activating<span class="w"> </span>OSD<span class="w"> </span>ID<span class="w"> </span><span class="m">7</span><span class="w"> </span>FSID<span class="w"> </span>ae41fc24-8408-4656-8310-58e00a2a146b
Running<span class="w"> </span>command:<span class="w"> </span>/usr/bin/mount<span class="w"> </span>-t<span class="w"> </span>tmpfs<span class="w"> </span>tmpfs<span class="w"> </span>/var/lib/ceph/osd/ceph-7
--&gt;<span class="w"> </span>Executable<span class="w"> </span>selinuxenabled<span class="w"> </span>not<span class="w"> </span><span class="k">in</span><span class="w"> </span>PATH:<span class="w"> </span>/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
Running<span class="w"> </span>command:<span class="w"> </span>/usr/bin/chown<span class="w"> </span>-R<span class="w"> </span>ceph:ceph<span class="w"> </span>/var/lib/ceph/osd/ceph-7
Running<span class="w"> </span>command:<span class="w"> </span>/usr/bin/ceph-bluestore-tool<span class="w"> </span>--cluster<span class="o">=</span>ceph<span class="w"> </span>prime-osd-dir<span class="w"> </span>--dev<span class="w"> </span>/dev/ceph-25d71cae-e648-4a95-ae1f-c45f8fa547d7/osd-block-ae41fc24-8408-4656-8310-58e00a2a146b<span class="w"> </span>--path<span class="w"> </span>/var/lib/ceph/osd/ceph-7<span class="w"> </span>--no-mon-config
Running<span class="w"> </span>command:<span class="w"> </span>/usr/bin/ln<span class="w"> </span>-snf<span class="w"> </span>/dev/ceph-25d71cae-e648-4a95-ae1f-c45f8fa547d7/osd-block-ae41fc24-8408-4656-8310-58e00a2a146b<span class="w"> </span>/var/lib/ceph/osd/ceph-7/block
Running<span class="w"> </span>command:<span class="w"> </span>/usr/bin/chown<span class="w"> </span>-h<span class="w"> </span>ceph:ceph<span class="w"> </span>/var/lib/ceph/osd/ceph-7/block
Running<span class="w"> </span>command:<span class="w"> </span>/usr/bin/chown<span class="w"> </span>-R<span class="w"> </span>ceph:ceph<span class="w"> </span>/dev/dm-0
Running<span class="w"> </span>command:<span class="w"> </span>/usr/bin/chown<span class="w"> </span>-R<span class="w"> </span>ceph:ceph<span class="w"> </span>/var/lib/ceph/osd/ceph-7
Running<span class="w"> </span>command:<span class="w"> </span>/usr/bin/systemctl<span class="w"> </span><span class="nb">enable</span><span class="w"> </span>ceph-volume@lvm-7-ae41fc24-8408-4656-8310-58e00a2a146b
<span class="w"> </span>stderr:<span class="w"> </span>Created<span class="w"> </span>symlink<span class="w"> </span>/etc/systemd/system/multi-user.target.wants/ceph-volume@lvm-7-ae41fc24-8408-4656-8310-58e00a2a146b.service<span class="w"> </span><span class="w"> </span>/lib/systemd/system/ceph-volume@.service.
Running<span class="w"> </span>command:<span class="w"> </span>/usr/bin/systemctl<span class="w"> </span><span class="nb">enable</span><span class="w"> </span>--runtime<span class="w"> </span>ceph-osd@7
<span class="w"> </span>stderr:<span class="w"> </span>Created<span class="w"> </span>symlink<span class="w"> </span>/run/systemd/system/ceph-osd.target.wants/ceph-osd@7.service<span class="w"> </span><span class="w"> </span>/lib/systemd/system/ceph-osd@.service.
Running<span class="w"> </span>command:<span class="w"> </span>/usr/bin/systemctl<span class="w"> </span>start<span class="w"> </span>ceph-osd@7
--&gt;<span class="w"> </span>ceph-volume<span class="w"> </span>lvm<span class="w"> </span>activate<span class="w"> </span>successful<span class="w"> </span><span class="k">for</span><span class="w"> </span>osd<span class="w"> </span>ID:<span class="w"> </span><span class="m">7</span>
--&gt;<span class="w"> </span>Activating<span class="w"> </span>OSD<span class="w"> </span>ID<span class="w"> </span><span class="m">3</span><span class="w"> </span>FSID<span class="w"> </span>e65f3c81-23de-4f81-af2b-be185f6fef42
Running<span class="w"> </span>command:<span class="w"> </span>/usr/bin/mount<span class="w"> </span>-t<span class="w"> </span>tmpfs<span class="w"> </span>tmpfs<span class="w"> </span>/var/lib/ceph/osd/ceph-3
--&gt;<span class="w"> </span>Executable<span class="w"> </span>selinuxenabled<span class="w"> </span>not<span class="w"> </span><span class="k">in</span><span class="w"> </span>PATH:<span class="w"> </span>/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
Running<span class="w"> </span>command:<span class="w"> </span>/usr/bin/chown<span class="w"> </span>-R<span class="w"> </span>ceph:ceph<span class="w"> </span>/var/lib/ceph/osd/ceph-3
Running<span class="w"> </span>command:<span class="w"> </span>/usr/bin/ceph-bluestore-tool<span class="w"> </span>--cluster<span class="o">=</span>ceph<span class="w"> </span>prime-osd-dir<span class="w"> </span>--dev<span class="w"> </span>/dev/ceph-3f428911-5213-461d-a5fd-029f673d7ad2/osd-block-e65f3c81-23de-4f81-af2b-be185f6fef42<span class="w"> </span>--path<span class="w"> </span>/var/lib/ceph/osd/ceph-3<span class="w"> </span>--no-mon-config
Running<span class="w"> </span>command:<span class="w"> </span>/usr/bin/ln<span class="w"> </span>-snf<span class="w"> </span>/dev/ceph-3f428911-5213-461d-a5fd-029f673d7ad2/osd-block-e65f3c81-23de-4f81-af2b-be185f6fef42<span class="w"> </span>/var/lib/ceph/osd/ceph-3/block
Running<span class="w"> </span>command:<span class="w"> </span>/usr/bin/chown<span class="w"> </span>-h<span class="w"> </span>ceph:ceph<span class="w"> </span>/var/lib/ceph/osd/ceph-3/block
Running<span class="w"> </span>command:<span class="w"> </span>/usr/bin/chown<span class="w"> </span>-R<span class="w"> </span>ceph:ceph<span class="w"> </span>/dev/dm-1
Running<span class="w"> </span>command:<span class="w"> </span>/usr/bin/chown<span class="w"> </span>-R<span class="w"> </span>ceph:ceph<span class="w"> </span>/var/lib/ceph/osd/ceph-3
Running<span class="w"> </span>command:<span class="w"> </span>/usr/bin/systemctl<span class="w"> </span><span class="nb">enable</span><span class="w"> </span>ceph-volume@lvm-3-e65f3c81-23de-4f81-af2b-be185f6fef42
<span class="w"> </span>stderr:<span class="w"> </span>Created<span class="w"> </span>symlink<span class="w"> </span>/etc/systemd/system/multi-user.target.wants/ceph-volume@lvm-3-e65f3c81-23de-4f81-af2b-be185f6fef42.service<span class="w"> </span><span class="w"> </span>/lib/systemd/system/ceph-volume@.service.
Running<span class="w"> </span>command:<span class="w"> </span>/usr/bin/systemctl<span class="w"> </span><span class="nb">enable</span><span class="w"> </span>--runtime<span class="w"> </span>ceph-osd@3
<span class="w"> </span>stderr:<span class="w"> </span>Created<span class="w"> </span>symlink<span class="w"> </span>/run/systemd/system/ceph-osd.target.wants/ceph-osd@3.service<span class="w"> </span><span class="w"> </span>/lib/systemd/system/ceph-osd@.service.
Running<span class="w"> </span>command:<span class="w"> </span>/usr/bin/systemctl<span class="w"> </span>start<span class="w"> </span>ceph-osd@3
--&gt;<span class="w"> </span>ceph-volume<span class="w"> </span>lvm<span class="w"> </span>activate<span class="w"> </span>successful<span class="w"> </span><span class="k">for</span><span class="w"> </span>osd<span class="w"> </span>ID:<span class="w"> </span><span class="m">3</span>
--&gt;<span class="w"> </span>Activating<span class="w"> </span>OSD<span class="w"> </span>ID<span class="w"> </span><span class="m">5</span><span class="w"> </span>FSID<span class="w"> </span>e06ae6d2-d881-40a0-adcd-2c55020474d5
Running<span class="w"> </span>command:<span class="w"> </span>/usr/bin/mount<span class="w"> </span>-t<span class="w"> </span>tmpfs<span class="w"> </span>tmpfs<span class="w"> </span>/var/lib/ceph/osd/ceph-5
--&gt;<span class="w"> </span>Executable<span class="w"> </span>selinuxenabled<span class="w"> </span>not<span class="w"> </span><span class="k">in</span><span class="w"> </span>PATH:<span class="w"> </span>/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
Running<span class="w"> </span>command:<span class="w"> </span>/usr/bin/chown<span class="w"> </span>-R<span class="w"> </span>ceph:ceph<span class="w"> </span>/var/lib/ceph/osd/ceph-5
Running<span class="w"> </span>command:<span class="w"> </span>/usr/bin/ceph-bluestore-tool<span class="w"> </span>--cluster<span class="o">=</span>ceph<span class="w"> </span>prime-osd-dir<span class="w"> </span>--dev<span class="w"> </span>/dev/ceph-8e2310de-b5ed-46bd-a32e-5048af46cfe0/osd-block-e06ae6d2-d881-40a0-adcd-2c55020474d5<span class="w"> </span>--path<span class="w"> </span>/var/lib/ceph/osd/ceph-5<span class="w"> </span>--no-mon-config
Running<span class="w"> </span>command:<span class="w"> </span>/usr/bin/ln<span class="w"> </span>-snf<span class="w"> </span>/dev/ceph-8e2310de-b5ed-46bd-a32e-5048af46cfe0/osd-block-e06ae6d2-d881-40a0-adcd-2c55020474d5<span class="w"> </span>/var/lib/ceph/osd/ceph-5/block
Running<span class="w"> </span>command:<span class="w"> </span>/usr/bin/chown<span class="w"> </span>-h<span class="w"> </span>ceph:ceph<span class="w"> </span>/var/lib/ceph/osd/ceph-5/block
Running<span class="w"> </span>command:<span class="w"> </span>/usr/bin/chown<span class="w"> </span>-R<span class="w"> </span>ceph:ceph<span class="w"> </span>/dev/dm-2
Running<span class="w"> </span>command:<span class="w"> </span>/usr/bin/chown<span class="w"> </span>-R<span class="w"> </span>ceph:ceph<span class="w"> </span>/var/lib/ceph/osd/ceph-5
Running<span class="w"> </span>command:<span class="w"> </span>/usr/bin/systemctl<span class="w"> </span><span class="nb">enable</span><span class="w"> </span>ceph-volume@lvm-5-e06ae6d2-d881-40a0-adcd-2c55020474d5
<span class="w"> </span>stderr:<span class="w"> </span>Created<span class="w"> </span>symlink<span class="w"> </span>/etc/systemd/system/multi-user.target.wants/ceph-volume@lvm-5-e06ae6d2-d881-40a0-adcd-2c55020474d5.service<span class="w"> </span><span class="w"> </span>/lib/systemd/system/ceph-volume@.service.
Running<span class="w"> </span>command:<span class="w"> </span>/usr/bin/systemctl<span class="w"> </span><span class="nb">enable</span><span class="w"> </span>--runtime<span class="w"> </span>ceph-osd@5
<span class="w"> </span>stderr:<span class="w"> </span>Created<span class="w"> </span>symlink<span class="w"> </span>/run/systemd/system/ceph-osd.target.wants/ceph-osd@5.service<span class="w"> </span><span class="w"> </span>/lib/systemd/system/ceph-osd@.service.
Running<span class="w"> </span>command:<span class="w"> </span>/usr/bin/systemctl<span class="w"> </span>start<span class="w"> </span>ceph-osd@5
</code></pre></div>
<p>Next, lets check the status of our OSDs using <code>ceph osd tree</code></p>
<div class="highlight"><pre><span></span><code>root@kube02:~#<span class="w"> </span>ceph<span class="w"> </span>osd<span class="w"> </span>tree
ID<span class="w">  </span>CLASS<span class="w">  </span>WEIGHT<span class="w">    </span>TYPE<span class="w"> </span>NAME<span class="w">        </span>STATUS<span class="w">  </span>REWEIGHT<span class="w">  </span>PRI-AFF
-1<span class="w">         </span><span class="m">14</span>.84685<span class="w">  </span>root<span class="w"> </span>default
-5<span class="w">          </span><span class="m">5</span>.24072<span class="w">      </span>host<span class="w"> </span>kube01
<span class="w"> </span><span class="m">4</span><span class="w">    </span>ssd<span class="w">   </span><span class="m">0</span>.87328<span class="w">          </span>osd.4<span class="w">        </span>up<span class="w">   </span><span class="m">1</span>.00000<span class="w">  </span><span class="m">1</span>.00000
<span class="w"> </span><span class="m">9</span><span class="w">    </span>ssd<span class="w">   </span><span class="m">0</span>.87328<span class="w">          </span>osd.9<span class="w">        </span>up<span class="w">   </span><span class="m">1</span>.00000<span class="w">  </span><span class="m">1</span>.00000
<span class="m">11</span><span class="w">    </span>ssd<span class="w">   </span><span class="m">1</span>.74709<span class="w">          </span>osd.11<span class="w">       </span>up<span class="w">   </span><span class="m">1</span>.00000<span class="w">  </span><span class="m">1</span>.00000
<span class="m">12</span><span class="w">    </span>ssd<span class="w">   </span><span class="m">1</span>.74709<span class="w">          </span>osd.12<span class="w">       </span>up<span class="w">   </span><span class="m">1</span>.00000<span class="w">  </span><span class="m">1</span>.00000
-7<span class="w">          </span><span class="m">3</span>.49310<span class="w">      </span>host<span class="w"> </span>kube02
<span class="w"> </span><span class="m">3</span><span class="w">    </span>ssd<span class="w">   </span><span class="m">0</span>.87328<span class="w">          </span>osd.3<span class="w">      </span>down<span class="w">   </span><span class="m">1</span>.00000<span class="w">  </span><span class="m">1</span>.00000
<span class="w"> </span><span class="m">5</span><span class="w">    </span>ssd<span class="w">   </span><span class="m">0</span>.87328<span class="w">          </span>osd.5<span class="w">      </span>down<span class="w">   </span><span class="m">1</span>.00000<span class="w">  </span><span class="m">1</span>.00000
<span class="w"> </span><span class="m">6</span><span class="w">    </span>ssd<span class="w">   </span><span class="m">0</span>.87328<span class="w">          </span>osd.6<span class="w">      </span>down<span class="w">   </span><span class="m">1</span>.00000<span class="w">  </span><span class="m">1</span>.00000
<span class="w"> </span><span class="m">7</span><span class="w">    </span>ssd<span class="w">   </span><span class="m">0</span>.87328<span class="w">          </span>osd.7<span class="w">      </span>down<span class="w">   </span><span class="m">1</span>.00000<span class="w">  </span><span class="m">1</span>.00000
-3<span class="w">          </span><span class="m">6</span>.11302<span class="w">      </span>host<span class="w"> </span>kube05
<span class="w"> </span><span class="m">0</span><span class="w">    </span>ssd<span class="w">   </span><span class="m">0</span>.87328<span class="w">          </span>osd.0<span class="w">        </span>up<span class="w">   </span><span class="m">1</span>.00000<span class="w">  </span><span class="m">1</span>.00000
<span class="w"> </span><span class="m">1</span><span class="w">    </span>ssd<span class="w">   </span><span class="m">0</span>.87328<span class="w">          </span>osd.1<span class="w">        </span>up<span class="w">   </span><span class="m">1</span>.00000<span class="w">  </span><span class="m">1</span>.00000
<span class="w"> </span><span class="m">2</span><span class="w">    </span>ssd<span class="w">   </span><span class="m">0</span>.87328<span class="w">          </span>osd.2<span class="w">        </span>up<span class="w">   </span><span class="m">1</span>.00000<span class="w">  </span><span class="m">1</span>.00000
<span class="w"> </span><span class="m">8</span><span class="w">    </span>ssd<span class="w">   </span><span class="m">1</span>.74660<span class="w">          </span>osd.8<span class="w">        </span>up<span class="w">   </span><span class="m">1</span>.00000<span class="w">  </span><span class="m">1</span>.00000
<span class="m">10</span><span class="w">    </span>ssd<span class="w">   </span><span class="m">1</span>.74660<span class="w">          </span>osd.10<span class="w">       </span>up<span class="w">   </span><span class="m">1</span>.00000<span class="w">  </span><span class="m">1</span>.00000
</code></pre></div>
<p>Oh, that is no good. They are still down. Lets fix that.</p>
<p>First- mark sure they are marked as up. (Use the number from the ID column in <code>ceph osd tree</code>)</p>
<div class="highlight"><pre><span></span><code>root@kube02:~#<span class="w"> </span>ceph<span class="w"> </span>osd<span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="m">3</span>
marked<span class="w"> </span><span class="k">in</span><span class="w"> </span>osd.3.
root@kube02:~#<span class="w"> </span>ceph<span class="w"> </span>osd<span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="m">5</span>
marked<span class="w"> </span><span class="k">in</span><span class="w"> </span>osd.5.
root@kube02:~#<span class="w"> </span>ceph<span class="w"> </span>osd<span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="m">6</span>
marked<span class="w"> </span><span class="k">in</span><span class="w"> </span>osd.6.
root@kube02:~#<span class="w"> </span>ceph<span class="w"> </span>osd<span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="m">7</span>
marked<span class="w"> </span><span class="k">in</span><span class="w"> </span>osd.7.
</code></pre></div>
<p>Next- try to start one.</p>
<div class="highlight"><pre><span></span><code>root@kube02:~#<span class="w"> </span>systemctl<span class="w"> </span>start<span class="w"> </span>ceph-osd@3
Job<span class="w"> </span><span class="k">for</span><span class="w"> </span>ceph-osd@3.service<span class="w"> </span>failed<span class="w"> </span>because<span class="w"> </span>the<span class="w"> </span>control<span class="w"> </span>process<span class="w"> </span>exited<span class="w"> </span>with<span class="w"> </span>error<span class="w"> </span>code.
See<span class="w"> </span><span class="s2">&quot;systemctl status ceph-osd@3.service&quot;</span><span class="w"> </span>and<span class="w"> </span><span class="s2">&quot;journalctl -xeu ceph-osd@3.service&quot;</span><span class="w"> </span><span class="k">for</span><span class="w"> </span>details.
</code></pre></div>
<p>Oops. That isn't good. Lets find out why.</p>
<div class="highlight"><pre><span></span><code>root@kube02:~#<span class="w"> </span>journalctl<span class="w"> </span>-xeu<span class="w"> </span>ceph-osd@3.service
<span class="w"> </span>Support:<span class="w"> </span>https://www.debian.org/support

<span class="w"> </span>A<span class="w"> </span>start<span class="w"> </span>job<span class="w"> </span><span class="k">for</span><span class="w"> </span>unit<span class="w"> </span>ceph-osd@3.service<span class="w"> </span>has<span class="w"> </span>finished<span class="w"> </span>successfully.

<span class="w"> </span>The<span class="w"> </span>job<span class="w"> </span>identifier<span class="w"> </span>is<span class="w"> </span><span class="m">8416</span>.
Jun<span class="w"> </span><span class="m">12</span><span class="w"> </span><span class="m">21</span>:16:12<span class="w"> </span>kube02<span class="w"> </span>ceph-osd<span class="o">[</span><span class="m">199292</span><span class="o">]</span>:<span class="w"> </span><span class="m">2024</span>-06-12T21:16:12.807-0500<span class="w"> </span>7042e2cc56c0<span class="w"> </span>-1<span class="w"> </span>unable<span class="w"> </span>to<span class="w"> </span>find<span class="w"> </span>any<span class="w"> </span>IPv4<span class="w"> </span>address<span class="w"> </span><span class="k">in</span><span class="w"> </span>networks<span class="w"> </span><span class="s1">&#39;10.100.6.105/24&#39;</span><span class="w"> </span>interfaces<span class="w"> </span><span class="s1">&#39;&#39;</span>
Jun<span class="w"> </span><span class="m">12</span><span class="w"> </span><span class="m">21</span>:16:12<span class="w"> </span>kube02<span class="w"> </span>ceph-osd<span class="o">[</span><span class="m">199292</span><span class="o">]</span>:<span class="w"> </span><span class="m">2024</span>-06-12T21:16:12.807-0500<span class="w"> </span>7042e2cc56c0<span class="w"> </span>-1<span class="w"> </span>Failed<span class="w"> </span>to<span class="w"> </span>pick<span class="w"> </span>cluster<span class="w"> </span>address.
Jun<span class="w"> </span><span class="m">12</span><span class="w"> </span><span class="m">21</span>:16:12<span class="w"> </span>kube02<span class="w"> </span>systemd<span class="o">[</span><span class="m">1</span><span class="o">]</span>:<span class="w"> </span>ceph-osd@3.service:<span class="w"> </span>Main<span class="w"> </span>process<span class="w"> </span>exited,<span class="w"> </span><span class="nv">code</span><span class="o">=</span>exited,<span class="w"> </span><span class="nv">status</span><span class="o">=</span><span class="m">1</span>/FAILURE
<span class="w"> </span>Subject:<span class="w"> </span>Unit<span class="w"> </span>process<span class="w"> </span>exited
<span class="w"> </span>Defined-By:<span class="w"> </span>systemd
<span class="w"> </span>Support:<span class="w"> </span>https://www.debian.org/support

<span class="w"> </span>An<span class="w"> </span><span class="nv">ExecStart</span><span class="o">=</span><span class="w"> </span>process<span class="w"> </span>belonging<span class="w"> </span>to<span class="w"> </span>unit<span class="w"> </span>ceph-osd@3.service<span class="w"> </span>has<span class="w"> </span>exited.

<span class="w"> </span>The<span class="w"> </span>process<span class="s1">&#39; exit code is &#39;</span>exited<span class="s1">&#39; and its exit status is 1.</span>
<span class="s1">Jun 12 21:16:12 kube02 systemd[1]: ceph-osd@3.service: Failed with result &#39;</span>exit-code<span class="s1">&#39;.</span>
<span class="s1"> Subject: Unit failed</span>
<span class="s1"> Defined-By: systemd</span>
<span class="s1"> Support: https://www.debian.org/support</span>
<span class="s1"></span>
<span class="s1"> The unit ceph-osd@3.service has entered the &#39;</span>failed<span class="s1">&#39; state with result &#39;</span>exit-code<span class="s1">&#39;.</span>
<span class="s1">Jun 12 21:16:23 kube02 systemd[1]: ceph-osd@3.service: Scheduled restart job, restart counter is at 4.</span>
<span class="s1"> Subject: Automatic restarting of a unit has been scheduled</span>
<span class="s1"> Defined-By: systemd</span>
<span class="s1"> Support: https://www.debian.org/support</span>
<span class="s1"></span>
<span class="s1"> Automatic restarting of the unit ceph-osd@3.service has been scheduled, as the result for</span>
<span class="s1"> the configured Restart= setting for the unit.</span>
<span class="s1">Jun 12 21:16:23 kube02 systemd[1]: Stopped ceph-osd@3.service - Ceph object storage daemon osd.3.</span>
<span class="s1"> Subject: A stop job for unit ceph-osd@3.service has finished</span>
<span class="s1"> Defined-By: systemd</span>
<span class="s1"> Support: https://www.debian.org/support</span>
<span class="s1"></span>
<span class="s1"> A stop job for unit ceph-osd@3.service has finished.</span>
<span class="s1"></span>
<span class="s1"> The job identifier is 8927 and the job result is done.</span>
<span class="s1">Jun 12 21:16:23 kube02 systemd[1]: ceph-osd@3.service: Start request repeated too quickly.</span>
<span class="s1">Jun 12 21:16:23 kube02 systemd[1]: ceph-osd@3.service: Failed with result &#39;</span>exit-code<span class="s1">&#39;.</span>
<span class="s1"> Subject: Unit failed</span>
<span class="s1"> Defined-By: systemd</span>
<span class="s1"> Support: https://www.debian.org/support</span>
<span class="s1"></span>
<span class="s1"> The unit ceph-osd@3.service has entered the &#39;</span>failed<span class="s1">&#39; state with result &#39;</span>exit-code<span class="s1">&#39;.</span>
<span class="s1">Jun 12 21:16:23 kube02 systemd[1]: Failed to start ceph-osd@3.service - Ceph object storage daemon osd.3.</span>
<span class="s1"> Subject: A start job for unit ceph-osd@3.service has failed</span>
<span class="s1"> Defined-By: systemd</span>
<span class="s1"> Support: https://www.debian.org/support</span>
<span class="s1"></span>
<span class="s1"> A start job for unit ceph-osd@3.service has finished with a failure.</span>
<span class="s1"></span>
<span class="s1"> The job identifier is 8927 and the job result is failed.</span>
<span class="s1">Jun 12 21:19:27 kube02 systemd[1]: ceph-osd@3.service: Start request repeated too quickly.</span>
<span class="s1">Jun 12 21:19:27 kube02 systemd[1]: ceph-osd@3.service: Failed with result &#39;</span>exit-code<span class="s1">&#39;.</span>
<span class="s1"> Subject: Unit failed</span>
<span class="s1"> Defined-By: systemd</span>
<span class="s1"> Support: https://www.debian.org/support</span>
<span class="s1"></span>
<span class="s1"> The unit ceph-osd@3.service has entered the &#39;</span>failed<span class="s1">&#39; state with result &#39;</span>exit-code<span class="err">&#39;</span>.
Jun<span class="w"> </span><span class="m">12</span><span class="w"> </span><span class="m">21</span>:19:27<span class="w"> </span>kube02<span class="w"> </span>systemd<span class="o">[</span><span class="m">1</span><span class="o">]</span>:<span class="w"> </span>Failed<span class="w"> </span>to<span class="w"> </span>start<span class="w"> </span>ceph-osd@3.service<span class="w"> </span>-<span class="w"> </span>Ceph<span class="w"> </span>object<span class="w"> </span>storage<span class="w"> </span>daemon<span class="w"> </span>osd.3.
<span class="w"> </span>Subject:<span class="w"> </span>A<span class="w"> </span>start<span class="w"> </span>job<span class="w"> </span><span class="k">for</span><span class="w"> </span>unit<span class="w"> </span>ceph-osd@3.service<span class="w"> </span>has<span class="w"> </span>failed
<span class="w"> </span>Defined-By:<span class="w"> </span>systemd
<span class="w"> </span>Support:<span class="w"> </span>https://www.debian.org/support

<span class="w"> </span>A<span class="w"> </span>start<span class="w"> </span>job<span class="w"> </span><span class="k">for</span><span class="w"> </span>unit<span class="w"> </span>ceph-osd@3.service<span class="w"> </span>has<span class="w"> </span>finished<span class="w"> </span>with<span class="w"> </span>a<span class="w"> </span>failure.

<span class="w"> </span>The<span class="w"> </span>job<span class="w"> </span>identifier<span class="w"> </span>is<span class="w"> </span><span class="m">9281</span><span class="w"> </span>and<span class="w"> </span>the<span class="w"> </span>job<span class="w"> </span>result<span class="w"> </span>is<span class="w"> </span>failed.
</code></pre></div>
<p>Near the top of the output, you may have noticed this line: <code>unable to find any IPv4 address in networks '10.100.6.0/24' interfaces</code></p>
<p>I have a dedicated non-routed layer 2 network specifically for ceph-traffic. It would seem, I need to go re-add that interface.</p>
<div class="highlight"><pre><span></span><code>nano<span class="w"> </span>/etc/network/interfaces

<span class="c1">## Make your changes to the networking file</span>

<span class="c1">## And- then, reload networking</span>
systemctl<span class="w"> </span>reload<span class="w"> </span>networking.service
</code></pre></div>
<p>If- after a few seconds, your bash prompt re-appears, its going to be a good day.</p>
<p>If not, and you messed up your networking configuration- your day just got a little bit longer.</p>
<p>In my case, I copied the networking file from one of my other hosts, as it has special bridge-vids, and settings for SDN (software defined networking). I updated the IP addresses, and network adapters, BUT, I forgot to update the vlan-raw-device under my vlan configuration. </p>
<div class="highlight"><pre><span></span><code>auto lo
iface lo inet loopback

auto eno1
#ConnectX-3 10G

iface eno1 inet manual
        mtu 1500

auto vmbr0
iface vmbr0 inet static
        address 10.100.4.102/24
        gateway 10.100.4.1
        bridge-ports eno1
        bridge-stp off
        bridge-fd 0
        bridge-vlan-aware yes
        bridge-vids 2-15 100 4040

auto vlan6
iface vlan6 inet static
        address 10.100.6.102/24
        vlan-raw-device eno1      # &lt;------ I forgot to update this
#V_STORAGE
source /etc/network/interfaces.d/*
</code></pre></div>
<p>At this point, lets go check our OSDs...</p>
<div class="highlight"><pre><span></span><code>root@kube02:~#<span class="w"> </span>ceph<span class="w"> </span>osd<span class="w"> </span>tree
ID<span class="w">  </span>CLASS<span class="w">  </span>WEIGHT<span class="w">    </span>TYPE<span class="w"> </span>NAME<span class="w">        </span>STATUS<span class="w">  </span>REWEIGHT<span class="w">  </span>PRI-AFF
-1<span class="w">         </span><span class="m">14</span>.84685<span class="w">  </span>root<span class="w"> </span>default
-5<span class="w">          </span><span class="m">5</span>.24072<span class="w">      </span>host<span class="w"> </span>kube01
<span class="w"> </span><span class="m">4</span><span class="w">    </span>ssd<span class="w">   </span><span class="m">0</span>.87328<span class="w">          </span>osd.4<span class="w">        </span>up<span class="w">   </span><span class="m">1</span>.00000<span class="w">  </span><span class="m">1</span>.00000
<span class="w"> </span><span class="m">9</span><span class="w">    </span>ssd<span class="w">   </span><span class="m">0</span>.87328<span class="w">          </span>osd.9<span class="w">        </span>up<span class="w">   </span><span class="m">1</span>.00000<span class="w">  </span><span class="m">1</span>.00000
<span class="m">11</span><span class="w">    </span>ssd<span class="w">   </span><span class="m">1</span>.74709<span class="w">          </span>osd.11<span class="w">       </span>up<span class="w">   </span><span class="m">1</span>.00000<span class="w">  </span><span class="m">1</span>.00000
<span class="m">12</span><span class="w">    </span>ssd<span class="w">   </span><span class="m">1</span>.74709<span class="w">          </span>osd.12<span class="w">       </span>up<span class="w">   </span><span class="m">1</span>.00000<span class="w">  </span><span class="m">1</span>.00000
-7<span class="w">          </span><span class="m">3</span>.49310<span class="w">      </span>host<span class="w"> </span>kube02
<span class="w"> </span><span class="m">3</span><span class="w">    </span>ssd<span class="w">   </span><span class="m">0</span>.87328<span class="w">          </span>osd.3<span class="w">      </span>down<span class="w">   </span><span class="m">1</span>.00000<span class="w">  </span><span class="m">1</span>.00000
<span class="w"> </span><span class="m">5</span><span class="w">    </span>ssd<span class="w">   </span><span class="m">0</span>.87328<span class="w">          </span>osd.5<span class="w">      </span>down<span class="w">   </span><span class="m">1</span>.00000<span class="w">  </span><span class="m">1</span>.00000
<span class="w"> </span><span class="m">6</span><span class="w">    </span>ssd<span class="w">   </span><span class="m">0</span>.87328<span class="w">          </span>osd.6<span class="w">      </span>down<span class="w">   </span><span class="m">1</span>.00000<span class="w">  </span><span class="m">1</span>.00000
<span class="w"> </span><span class="m">7</span><span class="w">    </span>ssd<span class="w">   </span><span class="m">0</span>.87328<span class="w">          </span>osd.7<span class="w">      </span>down<span class="w">   </span><span class="m">1</span>.00000<span class="w">  </span><span class="m">1</span>.00000
-3<span class="w">          </span><span class="m">6</span>.11302<span class="w">      </span>host<span class="w"> </span>kube05
<span class="w"> </span><span class="m">0</span><span class="w">    </span>ssd<span class="w">   </span><span class="m">0</span>.87328<span class="w">          </span>osd.0<span class="w">        </span>up<span class="w">   </span><span class="m">1</span>.00000<span class="w">  </span><span class="m">1</span>.00000
<span class="w"> </span><span class="m">1</span><span class="w">    </span>ssd<span class="w">   </span><span class="m">0</span>.87328<span class="w">          </span>osd.1<span class="w">        </span>up<span class="w">   </span><span class="m">1</span>.00000<span class="w">  </span><span class="m">1</span>.00000
<span class="w"> </span><span class="m">2</span><span class="w">    </span>ssd<span class="w">   </span><span class="m">0</span>.87328<span class="w">          </span>osd.2<span class="w">        </span>up<span class="w">   </span><span class="m">1</span>.00000<span class="w">  </span><span class="m">1</span>.00000
<span class="w"> </span><span class="m">8</span><span class="w">    </span>ssd<span class="w">   </span><span class="m">1</span>.74660<span class="w">          </span>osd.8<span class="w">        </span>up<span class="w">   </span><span class="m">1</span>.00000<span class="w">  </span><span class="m">1</span>.00000
<span class="m">10</span><span class="w">    </span>ssd<span class="w">   </span><span class="m">1</span>.74660<span class="w">          </span>osd.10<span class="w">       </span>up<span class="w">   </span><span class="m">1</span>.00000<span class="w">  </span><span class="m">1</span>.00000
</code></pre></div>
<p>Still down. Lets reset them, and try again.</p>
<div class="highlight"><pre><span></span><code>root@kube02:~#<span class="w"> </span>systemctl<span class="w"> </span>reset-failed<span class="w"> </span>ceph-osd@3.service
root@kube02:~#<span class="w"> </span>systemctl<span class="w"> </span>reset-failed<span class="w"> </span>ceph-osd@5.service
root@kube02:~#<span class="w"> </span>systemctl<span class="w"> </span>reset-failed<span class="w"> </span>ceph-osd@6.service
root@kube02:~#<span class="w"> </span>systemctl<span class="w"> </span>reset-failed<span class="w"> </span>ceph-osd@7.service
root@kube02:~#<span class="w"> </span>systemctl<span class="w"> </span>start<span class="w"> </span>ceph-osd@3.service
root@kube02:~#<span class="w"> </span>systemctl<span class="w"> </span>start<span class="w"> </span>ceph-osd@5.service
root@kube02:~#<span class="w"> </span>systemctl<span class="w"> </span>start<span class="w"> </span>ceph-osd@6.service
root@kube02:~#<span class="w"> </span>systemctl<span class="w"> </span>start<span class="w"> </span>ceph-osd@7.service
</code></pre></div>
<p><code>ceph osd tree</code> won't update instantly. </p>
<p>We can, check <code>journalctl -fe</code> for errors.</p>
<div class="highlight"><pre><span></span><code>root@kube02:~#<span class="w"> </span>journalctl<span class="w"> </span>-fe
...
Jun<span class="w"> </span><span class="m">12</span><span class="w"> </span><span class="m">21</span>:30:12<span class="w"> </span>kube02<span class="w"> </span>systemd<span class="o">[</span><span class="m">1</span><span class="o">]</span>:<span class="w"> </span>Starting<span class="w"> </span>ceph-osd@5.service<span class="w"> </span>-<span class="w"> </span>Ceph<span class="w"> </span>object<span class="w"> </span>storage<span class="w"> </span>daemon<span class="w"> </span>osd.5...
Jun<span class="w"> </span><span class="m">12</span><span class="w"> </span><span class="m">21</span>:30:13<span class="w"> </span>kube02<span class="w"> </span>systemd<span class="o">[</span><span class="m">1</span><span class="o">]</span>:<span class="w"> </span>Started<span class="w"> </span>ceph-osd@5.service<span class="w"> </span>-<span class="w"> </span>Ceph<span class="w"> </span>object<span class="w"> </span>storage<span class="w"> </span>daemon<span class="w"> </span>osd.5.
Jun<span class="w"> </span><span class="m">12</span><span class="w"> </span><span class="m">21</span>:30:13<span class="w"> </span>kube02<span class="w"> </span>systemd<span class="o">[</span><span class="m">1</span><span class="o">]</span>:<span class="w"> </span>Starting<span class="w"> </span>ceph-osd@6.service<span class="w"> </span>-<span class="w"> </span>Ceph<span class="w"> </span>object<span class="w"> </span>storage<span class="w"> </span>daemon<span class="w"> </span>osd.6...
Jun<span class="w"> </span><span class="m">12</span><span class="w"> </span><span class="m">21</span>:30:15<span class="w"> </span>kube02<span class="w"> </span>systemd<span class="o">[</span><span class="m">1</span><span class="o">]</span>:<span class="w"> </span>Started<span class="w"> </span>ceph-osd@6.service<span class="w"> </span>-<span class="w"> </span>Ceph<span class="w"> </span>object<span class="w"> </span>storage<span class="w"> </span>daemon<span class="w"> </span>osd.6.
Jun<span class="w"> </span><span class="m">12</span><span class="w"> </span><span class="m">21</span>:30:16<span class="w"> </span>kube02<span class="w"> </span>systemd<span class="o">[</span><span class="m">1</span><span class="o">]</span>:<span class="w"> </span>Starting<span class="w"> </span>ceph-osd@7.service<span class="w"> </span>-<span class="w"> </span>Ceph<span class="w"> </span>object<span class="w"> </span>storage<span class="w"> </span>daemon<span class="w"> </span>osd.7...
Jun<span class="w"> </span><span class="m">12</span><span class="w"> </span><span class="m">21</span>:30:17<span class="w"> </span>kube02<span class="w"> </span>systemd<span class="o">[</span><span class="m">1</span><span class="o">]</span>:<span class="w"> </span>Started<span class="w"> </span>ceph-osd@7.service<span class="w"> </span>-<span class="w"> </span>Ceph<span class="w"> </span>object<span class="w"> </span>storage<span class="w"> </span>daemon<span class="w"> </span>osd.7.
...
</code></pre></div>
<p>No errors, should be all good.</p>
<div class="highlight"><pre><span></span><code><span class="c1"># After a few moments... we had one OSD up.</span>
root@kube02:~#<span class="w"> </span>ceph<span class="w"> </span>osd<span class="w"> </span>tree
ID<span class="w">  </span>CLASS<span class="w">  </span>WEIGHT<span class="w">    </span>TYPE<span class="w"> </span>NAME<span class="w">        </span>STATUS<span class="w">  </span>REWEIGHT<span class="w">  </span>PRI-AFF
-1<span class="w">         </span><span class="m">14</span>.84685<span class="w">  </span>root<span class="w"> </span>default
-5<span class="w">          </span><span class="m">5</span>.24072<span class="w">      </span>host<span class="w"> </span>kube01
<span class="w"> </span><span class="m">4</span><span class="w">    </span>ssd<span class="w">   </span><span class="m">0</span>.87328<span class="w">          </span>osd.4<span class="w">        </span>up<span class="w">   </span><span class="m">1</span>.00000<span class="w">  </span><span class="m">1</span>.00000
<span class="w"> </span><span class="m">9</span><span class="w">    </span>ssd<span class="w">   </span><span class="m">0</span>.87328<span class="w">          </span>osd.9<span class="w">        </span>up<span class="w">   </span><span class="m">1</span>.00000<span class="w">  </span><span class="m">1</span>.00000
<span class="m">11</span><span class="w">    </span>ssd<span class="w">   </span><span class="m">1</span>.74709<span class="w">          </span>osd.11<span class="w">       </span>up<span class="w">   </span><span class="m">1</span>.00000<span class="w">  </span><span class="m">1</span>.00000
<span class="m">12</span><span class="w">    </span>ssd<span class="w">   </span><span class="m">1</span>.74709<span class="w">          </span>osd.12<span class="w">       </span>up<span class="w">   </span><span class="m">1</span>.00000<span class="w">  </span><span class="m">1</span>.00000
-7<span class="w">          </span><span class="m">3</span>.49310<span class="w">      </span>host<span class="w"> </span>kube02
<span class="w"> </span><span class="m">3</span><span class="w">    </span>ssd<span class="w">   </span><span class="m">0</span>.87328<span class="w">          </span>osd.3<span class="w">      </span>down<span class="w">         </span><span class="m">0</span><span class="w">  </span><span class="m">1</span>.00000
<span class="w"> </span><span class="m">5</span><span class="w">    </span>ssd<span class="w">   </span><span class="m">0</span>.87328<span class="w">          </span>osd.5<span class="w">      </span>down<span class="w">         </span><span class="m">0</span><span class="w">  </span><span class="m">1</span>.00000
<span class="w"> </span><span class="m">6</span><span class="w">    </span>ssd<span class="w">   </span><span class="m">0</span>.87328<span class="w">          </span>osd.6<span class="w">      </span>down<span class="w">         </span><span class="m">0</span><span class="w">  </span><span class="m">1</span>.00000
<span class="w"> </span><span class="m">7</span><span class="w">    </span>ssd<span class="w">   </span><span class="m">0</span>.87328<span class="w">          </span>osd.7<span class="w">        </span>up<span class="w">   </span><span class="m">1</span>.00000<span class="w">  </span><span class="m">1</span>.00000
-3<span class="w">          </span><span class="m">6</span>.11302<span class="w">      </span>host<span class="w"> </span>kube05
<span class="w"> </span><span class="m">0</span><span class="w">    </span>ssd<span class="w">   </span><span class="m">0</span>.87328<span class="w">          </span>osd.0<span class="w">        </span>up<span class="w">   </span><span class="m">1</span>.00000<span class="w">  </span><span class="m">1</span>.00000
<span class="w"> </span><span class="m">1</span><span class="w">    </span>ssd<span class="w">   </span><span class="m">0</span>.87328<span class="w">          </span>osd.1<span class="w">        </span>up<span class="w">   </span><span class="m">1</span>.00000<span class="w">  </span><span class="m">1</span>.00000
<span class="w"> </span><span class="m">2</span><span class="w">    </span>ssd<span class="w">   </span><span class="m">0</span>.87328<span class="w">          </span>osd.2<span class="w">        </span>up<span class="w">   </span><span class="m">1</span>.00000<span class="w">  </span><span class="m">1</span>.00000
<span class="w"> </span><span class="m">8</span><span class="w">    </span>ssd<span class="w">   </span><span class="m">1</span>.74660<span class="w">          </span>osd.8<span class="w">        </span>up<span class="w">   </span><span class="m">1</span>.00000<span class="w">  </span><span class="m">1</span>.00000
<span class="m">10</span><span class="w">    </span>ssd<span class="w">   </span><span class="m">1</span>.74660<span class="w">          </span>osd.10<span class="w">       </span>up<span class="w">   </span><span class="m">1</span>.00000<span class="w">  </span><span class="m">1</span>.00000

<span class="c1">## And- after a couple of minutes, everything was up.</span>
root@kube02:~#<span class="w"> </span>ceph<span class="w"> </span>osd<span class="w"> </span>tree
ID<span class="w">  </span>CLASS<span class="w">  </span>WEIGHT<span class="w">    </span>TYPE<span class="w"> </span>NAME<span class="w">        </span>STATUS<span class="w">  </span>REWEIGHT<span class="w">  </span>PRI-AFF
-1<span class="w">         </span><span class="m">14</span>.84685<span class="w">  </span>root<span class="w"> </span>default
-5<span class="w">          </span><span class="m">5</span>.24072<span class="w">      </span>host<span class="w"> </span>kube01
<span class="w"> </span><span class="m">4</span><span class="w">    </span>ssd<span class="w">   </span><span class="m">0</span>.87328<span class="w">          </span>osd.4<span class="w">        </span>up<span class="w">   </span><span class="m">1</span>.00000<span class="w">  </span><span class="m">1</span>.00000
<span class="w"> </span><span class="m">9</span><span class="w">    </span>ssd<span class="w">   </span><span class="m">0</span>.87328<span class="w">          </span>osd.9<span class="w">        </span>up<span class="w">   </span><span class="m">1</span>.00000<span class="w">  </span><span class="m">1</span>.00000
<span class="m">11</span><span class="w">    </span>ssd<span class="w">   </span><span class="m">1</span>.74709<span class="w">          </span>osd.11<span class="w">       </span>up<span class="w">   </span><span class="m">1</span>.00000<span class="w">  </span><span class="m">1</span>.00000
<span class="m">12</span><span class="w">    </span>ssd<span class="w">   </span><span class="m">1</span>.74709<span class="w">          </span>osd.12<span class="w">       </span>up<span class="w">   </span><span class="m">1</span>.00000<span class="w">  </span><span class="m">1</span>.00000
-7<span class="w">          </span><span class="m">3</span>.49310<span class="w">      </span>host<span class="w"> </span>kube02
<span class="w"> </span><span class="m">3</span><span class="w">    </span>ssd<span class="w">   </span><span class="m">0</span>.87328<span class="w">          </span>osd.3<span class="w">        </span>up<span class="w">   </span><span class="m">1</span>.00000<span class="w">  </span><span class="m">1</span>.00000
<span class="w"> </span><span class="m">5</span><span class="w">    </span>ssd<span class="w">   </span><span class="m">0</span>.87328<span class="w">          </span>osd.5<span class="w">        </span>up<span class="w">   </span><span class="m">1</span>.00000<span class="w">  </span><span class="m">1</span>.00000
<span class="w"> </span><span class="m">6</span><span class="w">    </span>ssd<span class="w">   </span><span class="m">0</span>.87328<span class="w">          </span>osd.6<span class="w">        </span>up<span class="w">   </span><span class="m">1</span>.00000<span class="w">  </span><span class="m">1</span>.00000
<span class="w"> </span><span class="m">7</span><span class="w">    </span>ssd<span class="w">   </span><span class="m">0</span>.87328<span class="w">          </span>osd.7<span class="w">        </span>up<span class="w">   </span><span class="m">1</span>.00000<span class="w">  </span><span class="m">1</span>.00000
-3<span class="w">          </span><span class="m">6</span>.11302<span class="w">      </span>host<span class="w"> </span>kube05
<span class="w"> </span><span class="m">0</span><span class="w">    </span>ssd<span class="w">   </span><span class="m">0</span>.87328<span class="w">          </span>osd.0<span class="w">        </span>up<span class="w">   </span><span class="m">1</span>.00000<span class="w">  </span><span class="m">1</span>.00000
<span class="w"> </span><span class="m">1</span><span class="w">    </span>ssd<span class="w">   </span><span class="m">0</span>.87328<span class="w">          </span>osd.1<span class="w">        </span>up<span class="w">   </span><span class="m">1</span>.00000<span class="w">  </span><span class="m">1</span>.00000
<span class="w"> </span><span class="m">2</span><span class="w">    </span>ssd<span class="w">   </span><span class="m">0</span>.87328<span class="w">          </span>osd.2<span class="w">        </span>up<span class="w">   </span><span class="m">1</span>.00000<span class="w">  </span><span class="m">1</span>.00000
<span class="w"> </span><span class="m">8</span><span class="w">    </span>ssd<span class="w">   </span><span class="m">1</span>.74660<span class="w">          </span>osd.8<span class="w">        </span>up<span class="w">   </span><span class="m">1</span>.00000<span class="w">  </span><span class="m">1</span>.00000
<span class="m">10</span><span class="w">    </span>ssd<span class="w">   </span><span class="m">1</span>.74660<span class="w">          </span>osd.10<span class="w">       </span>up<span class="w">   </span><span class="m">1</span>.00000<span class="w">  </span><span class="m">1</span>.00000
</code></pre></div>
<p>At this point, check <code>ceph status</code> If you want to watch the output as it heals up, you can use <code>ceph status -w</code> to watch the output.</p>
<div class="highlight"><pre><span></span><code>root@kube02:~#<span class="w"> </span>ceph<span class="w"> </span>status<span class="w"> </span>-w
<span class="w">  </span>cluster:
<span class="w">    </span>id:<span class="w">     </span>016d27bb-5e11-4c85-846a-98f1f0b7ec08
<span class="w">    </span>health:<span class="w"> </span>HEALTH_WARN
<span class="w">            </span>Degraded<span class="w"> </span>data<span class="w"> </span>redundancy:<span class="w"> </span><span class="m">288792</span>/1798224<span class="w"> </span>objects<span class="w"> </span>degraded<span class="w"> </span><span class="o">(</span><span class="m">16</span>.060%<span class="o">)</span>,<span class="w"> </span><span class="m">61</span><span class="w"> </span>pgs<span class="w"> </span>degraded,<span class="w"> </span><span class="m">42</span><span class="w"> </span>pgs<span class="w"> </span>undersized

<span class="w">  </span>services:
<span class="w">    </span>mon:<span class="w"> </span><span class="m">2</span><span class="w"> </span>daemons,<span class="w"> </span>quorum<span class="w"> </span>kube05,kube01<span class="w"> </span><span class="o">(</span>age<span class="w"> </span>3h<span class="o">)</span>
<span class="w">    </span>mgr:<span class="w"> </span>kube05<span class="o">(</span>active,<span class="w"> </span>since<span class="w"> </span>5h<span class="o">)</span>,<span class="w"> </span>standbys:<span class="w"> </span>kube01
<span class="w">    </span>mds:<span class="w"> </span><span class="m">1</span>/1<span class="w"> </span>daemons<span class="w"> </span>up,<span class="w"> </span><span class="m">1</span><span class="w"> </span>standby
<span class="w">    </span>osd:<span class="w"> </span><span class="m">13</span><span class="w"> </span>osds:<span class="w"> </span><span class="m">13</span><span class="w"> </span>up<span class="w"> </span><span class="o">(</span>since<span class="w"> </span>61s<span class="o">)</span>,<span class="w"> </span><span class="m">13</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="o">(</span>since<span class="w"> </span>61s<span class="o">)</span><span class="p">;</span><span class="w"> </span><span class="m">61</span><span class="w"> </span>remapped<span class="w"> </span>pgs

<span class="w">  </span>data:
<span class="w">    </span>volumes:<span class="w"> </span><span class="m">1</span>/1<span class="w"> </span>healthy
<span class="w">    </span>pools:<span class="w">   </span><span class="m">5</span><span class="w"> </span>pools,<span class="w"> </span><span class="m">129</span><span class="w"> </span>pgs
<span class="w">    </span>objects:<span class="w"> </span><span class="m">599</span>.41k<span class="w"> </span>objects,<span class="w"> </span><span class="m">1</span>.3<span class="w"> </span>TiB
<span class="w">    </span>usage:<span class="w">   </span><span class="m">3</span>.8<span class="w"> </span>TiB<span class="w"> </span>used,<span class="w"> </span><span class="m">11</span><span class="w"> </span>TiB<span class="w"> </span>/<span class="w"> </span><span class="m">15</span><span class="w"> </span>TiB<span class="w"> </span>avail
<span class="w">    </span>pgs:<span class="w">     </span><span class="m">288792</span>/1798224<span class="w"> </span>objects<span class="w"> </span>degraded<span class="w"> </span><span class="o">(</span><span class="m">16</span>.060%<span class="o">)</span>
<span class="w">             </span><span class="m">68</span><span class="w"> </span>active+clean
<span class="w">             </span><span class="m">39</span><span class="w"> </span>active+undersized+degraded+remapped+backfill_wait
<span class="w">             </span><span class="m">20</span><span class="w"> </span>active+recovery_wait+undersized+degraded+remapped
<span class="w">             </span><span class="m">2</span><span class="w">  </span>active+recovering+undersized+degraded+remapped

<span class="w">  </span>io:
<span class="w">    </span>client:<span class="w">   </span><span class="m">328</span><span class="w"> </span>KiB/s<span class="w"> </span>rd,<span class="w"> </span><span class="m">1</span>.9<span class="w"> </span>MiB/s<span class="w"> </span>wr,<span class="w"> </span><span class="m">83</span><span class="w"> </span>op/s<span class="w"> </span>rd,<span class="w"> </span><span class="m">154</span><span class="w"> </span>op/s<span class="w"> </span>wr
<span class="w">    </span>recovery:<span class="w"> </span><span class="m">21</span><span class="w"> </span>MiB/s,<span class="w"> </span><span class="m">59</span><span class="w"> </span>objects/s


<span class="m">2024</span>-06-12T21:33:54.022362-0500<span class="w"> </span>mon.kube05<span class="w"> </span><span class="o">[</span>WRN<span class="o">]</span><span class="w"> </span>Health<span class="w"> </span>check<span class="w"> </span>update:<span class="w"> </span>Degraded<span class="w"> </span>data<span class="w"> </span>redundancy:<span class="w"> </span><span class="m">288765</span>/1798227<span class="w"> </span>objects<span class="w"> </span>degraded<span class="w"> </span><span class="o">(</span><span class="m">16</span>.058%<span class="o">)</span>,<span class="w"> </span><span class="m">61</span><span class="w"> </span>pgs<span class="w"> </span>degraded,<span class="w"> </span><span class="m">61</span><span class="w"> </span>pgs<span class="w"> </span>undersized<span class="w"> </span><span class="o">(</span>PG_DEGRADED<span class="o">)</span>
<span class="m">2024</span>-06-12T21:33:59.025195-0500<span class="w"> </span>mon.kube05<span class="w"> </span><span class="o">[</span>WRN<span class="o">]</span><span class="w"> </span>Health<span class="w"> </span>check<span class="w"> </span>update:<span class="w"> </span>Degraded<span class="w"> </span>data<span class="w"> </span>redundancy:<span class="w"> </span><span class="m">288407</span>/1798227<span class="w"> </span>objects<span class="w"> </span>degraded<span class="w"> </span><span class="o">(</span><span class="m">16</span>.038%<span class="o">)</span>,<span class="w"> </span><span class="m">60</span><span class="w"> </span>pgs<span class="w"> </span>degraded,<span class="w"> </span><span class="m">60</span><span class="w"> </span>pgs<span class="w"> </span>undersized<span class="w"> </span><span class="o">(</span>PG_DEGRADED<span class="o">)</span>
<span class="m">2024</span>-06-12T21:34:04.030359-0500<span class="w"> </span>mon.kube05<span class="w"> </span><span class="o">[</span>WRN<span class="o">]</span><span class="w"> </span>Health<span class="w"> </span>check<span class="w"> </span>update:<span class="w"> </span>Degraded<span class="w"> </span>data<span class="w"> </span>redundancy:<span class="w"> </span><span class="m">288263</span>/1798227<span class="w"> </span>objects<span class="w"> </span>degraded<span class="w"> </span><span class="o">(</span><span class="m">16</span>.030%<span class="o">)</span>,<span class="w"> </span><span class="m">60</span><span class="w"> </span>pgs<span class="w"> </span>degraded,<span class="w"> </span><span class="m">60</span><span class="w"> </span>pgs<span class="w"> </span>undersized<span class="w"> </span><span class="o">(</span>PG_DEGRADED<span class="o">)</span>
<span class="m">2024</span>-06-12T21:34:09.033479-0500<span class="w"> </span>mon.kube05<span class="w"> </span><span class="o">[</span>WRN<span class="o">]</span><span class="w"> </span>Health<span class="w"> </span>check<span class="w"> </span>update:<span class="w"> </span>Degraded<span class="w"> </span>data<span class="w"> </span>redundancy:<span class="w"> </span><span class="m">287886</span>/1798227<span class="w"> </span>objects<span class="w"> </span>degraded<span class="w"> </span><span class="o">(</span><span class="m">16</span>.009%<span class="o">)</span>,<span class="w"> </span><span class="m">60</span><span class="w"> </span>pgs<span class="w"> </span>degraded,<span class="w"> </span><span class="m">60</span><span class="w"> </span>pgs<span class="w"> </span>undersized<span class="w"> </span><span class="o">(</span>PG_DEGRADED<span class="o">)</span>
<span class="m">2024</span>-06-12T21:34:14.037276-0500<span class="w"> </span>mon.kube05<span class="w"> </span><span class="o">[</span>WRN<span class="o">]</span><span class="w"> </span>Health<span class="w"> </span>check<span class="w"> </span>update:<span class="w"> </span>Degraded<span class="w"> </span>data<span class="w"> </span>redundancy:<span class="w"> </span><span class="m">287654</span>/1798230<span class="w"> </span>objects<span class="w"> </span>degraded<span class="w"> </span><span class="o">(</span><span class="m">15</span>.997%<span class="o">)</span>,<span class="w"> </span><span class="m">59</span><span class="w"> </span>pgs<span class="w"> </span>degraded,<span class="w"> </span><span class="m">59</span><span class="w"> </span>pgs<span class="w"> </span>undersized<span class="w"> </span><span class="o">(</span>PG_DEGRADED<span class="o">)</span>
<span class="m">2024</span>-06-12T21:34:19.040047-0500<span class="w"> </span>mon.kube05<span class="w"> </span><span class="o">[</span>WRN<span class="o">]</span><span class="w"> </span>Health<span class="w"> </span>check<span class="w"> </span>update:<span class="w"> </span>Degraded<span class="w"> </span>data<span class="w"> </span>redundancy:<span class="w"> </span><span class="m">287336</span>/1798230<span class="w"> </span>objects<span class="w"> </span>degraded<span class="w"> </span><span class="o">(</span><span class="m">15</span>.979%<span class="o">)</span>,<span class="w"> </span><span class="m">59</span><span class="w"> </span>pgs<span class="w"> </span>degraded,<span class="w"> </span><span class="m">59</span><span class="w"> </span>pgs<span class="w"> </span>undersized<span class="w"> </span><span class="o">(</span>PG_DEGRADED<span class="o">)</span>
<span class="m">2024</span>-06-12T21:34:24.043003-0500<span class="w"> </span>mon.kube05<span class="w"> </span><span class="o">[</span>WRN<span class="o">]</span><span class="w"> </span>Health<span class="w"> </span>check<span class="w"> </span>update:<span class="w"> </span>Degraded<span class="w"> </span>data<span class="w"> </span>redundancy:<span class="w"> </span><span class="m">287029</span>/1798251<span class="w"> </span>objects<span class="w"> </span>degraded<span class="w"> </span><span class="o">(</span><span class="m">15</span>.962%<span class="o">)</span>,<span class="w"> </span><span class="m">58</span><span class="w"> </span>pgs<span class="w"> </span>degraded,<span class="w"> </span><span class="m">58</span><span class="w"> </span>pgs<span class="w"> </span>undersized<span class="w"> </span><span class="o">(</span>PG_DEGRADED<span class="o">)</span>
<span class="m">2024</span>-06-12T21:34:29.047180-0500<span class="w"> </span>mon.kube05<span class="w"> </span><span class="o">[</span>WRN<span class="o">]</span><span class="w"> </span>Health<span class="w"> </span>check<span class="w"> </span>update:<span class="w"> </span>Degraded<span class="w"> </span>data<span class="w"> </span>redundancy:<span class="w"> </span><span class="m">286728</span>/1798263<span class="w"> </span>objects<span class="w"> </span>degraded<span class="w"> </span><span class="o">(</span><span class="m">15</span>.945%<span class="o">)</span>,<span class="w"> </span><span class="m">58</span><span class="w"> </span>pgs<span class="w"> </span>degraded,<span class="w"> </span><span class="m">58</span><span class="w"> </span>pgs<span class="w"> </span>undersized<span class="w"> </span><span class="o">(</span>PG_DEGRADED<span class="o">)</span>
</code></pre></div>
<p>The important thing to look for in this output- is the services section.</p>
<p>Make sure you have mons up, and quorum is established.</p>
<p>Make sure you a mgr up.</p>
<p>And, make sure all of your osds are up.</p>
<p>Otherwise, ceph will scan the OSDs, make changes as needed, and self-heal.</p>
<h4 id="step-9-re-import-sdn">Step 9. Re-import SDN<a class="headerlink" href="#step-9-re-import-sdn" title="Permanent link">&para;</a></h4>
<p>If, you use <a href="https://pve.proxmox.com/pve-docs/chapter-pvesdn.html" target="_blank">Proxmox's SDN Features</a>, there is a good chance this will not be applied to your node currently.</p>
<p>To correct this, you can do one of two things.</p>
<ol>
<li>Make a change in your SDN configuration, which will cause the new install to receive the update.</li>
<li>OR, run this command: <code>pvesh set /nodes/$(hostname)/network</code></li>
</ol>
<div class="highlight"><pre><span></span><code>root@kube02:~#<span class="w"> </span>pvesh<span class="w"> </span><span class="nb">set</span><span class="w"> </span>/nodes/<span class="k">$(</span>hostname<span class="k">)</span>/network
info:<span class="w"> </span>executing<span class="w"> </span>/usr/bin/dpkg<span class="w"> </span>-l<span class="w"> </span>ifupdown2
UPID:kube02:0003BA42:00137BD5:666A5E7A:srvreload:networking:root@pam:
</code></pre></div>
<p>Afterwards, all of your SDN configurations will be reapplied.</p>
<h4 id="step-10-regenerate-certs-and-remove-old-ssh-keys">Step 10. Regenerate certs, and remove old ssh keys<a class="headerlink" href="#step-10-regenerate-certs-and-remove-old-ssh-keys" title="Permanent link">&para;</a></h4>
<p>While- mostly everything is currently functional, if you attempt to open the console one of your VMs, you may noticed this:</p>
<div class="highlight"><pre><span></span><code>@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
@<span class="w">    </span>WARNING:<span class="w"> </span>REMOTE<span class="w"> </span>HOST<span class="w"> </span>IDENTIFICATION<span class="w"> </span>HAS<span class="w"> </span>CHANGED!<span class="w">     </span>@
@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
IT<span class="w"> </span>IS<span class="w"> </span>POSSIBLE<span class="w"> </span>THAT<span class="w"> </span>SOMEONE<span class="w"> </span>IS<span class="w"> </span>DOING<span class="w"> </span>SOMETHING<span class="w"> </span>NASTY!
Someone<span class="w"> </span>could<span class="w"> </span>be<span class="w"> </span>eavesdropping<span class="w"> </span>on<span class="w"> </span>you<span class="w"> </span>right<span class="w"> </span>now<span class="w"> </span><span class="o">(</span>man-in-the-middle<span class="w"> </span>attack<span class="o">)</span>!
It<span class="w"> </span>is<span class="w"> </span>also<span class="w"> </span>possible<span class="w"> </span>that<span class="w"> </span>a<span class="w"> </span>host<span class="w"> </span>key<span class="w"> </span>has<span class="w"> </span>just<span class="w"> </span>been<span class="w"> </span>changed.
The<span class="w"> </span>fingerprint<span class="w"> </span><span class="k">for</span><span class="w"> </span>the<span class="w"> </span>ECDSA<span class="w"> </span>key<span class="w"> </span>sent<span class="w"> </span>by<span class="w"> </span>the<span class="w"> </span>remote<span class="w"> </span>host<span class="w"> </span>is
SHA256:fxUoUrMalniKJQXOdrO0m9YAf03OXaa0gwQLaU0uwKA.
Please<span class="w"> </span>contact<span class="w"> </span>your<span class="w"> </span>system<span class="w"> </span>administrator.
Add<span class="w"> </span>correct<span class="w"> </span>host<span class="w"> </span>key<span class="w"> </span><span class="k">in</span><span class="w"> </span>/root/.ssh/known_hosts<span class="w"> </span>to<span class="w"> </span>get<span class="w"> </span>rid<span class="w"> </span>of<span class="w"> </span>this<span class="w"> </span>message.
Offending<span class="w"> </span>RSA<span class="w"> </span>key<span class="w"> </span><span class="k">in</span><span class="w"> </span>/etc/ssh/ssh_known_hosts:2
<span class="w">  </span>remove<span class="w"> </span>with:
<span class="w">  </span>ssh-keygen<span class="w"> </span>-f<span class="w"> </span><span class="s2">&quot;/etc/ssh/ssh_known_hosts&quot;</span><span class="w"> </span>-R<span class="w"> </span><span class="s2">&quot;10.100.4.102&quot;</span>
Host<span class="w"> </span>key<span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="m">10</span>.100.4.102<span class="w"> </span>has<span class="w"> </span>changed<span class="w"> </span>and<span class="w"> </span>you<span class="w"> </span>have<span class="w"> </span>requested<span class="w"> </span>strict<span class="w"> </span>checking.
Host<span class="w"> </span>key<span class="w"> </span>verification<span class="w"> </span>failed.
</code></pre></div>
<p>To fix this, we just need to re-generate the keys.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Do NOT run <code>ssh-keygen -f</code>! </p>
<p>It is a symlink to <code>ssh_known_hosts -&gt; /etc/pve/priv/known_hosts</code></p>
<p><code>ssh-keygen</code> will break this, and it will not be automatically synced and updated.</p>
<p>Proxmox links the ssh_known_hosts file back to its cluster file system, and all hosts "share" the same file.</p>
<p>BUG: https://bugzilla.proxmox.com/show_bug.cgi?id=4252</p>
</div>
<p>On the new host, run <code>pvecm updatecerts</code></p>
<p>Documentation: https://pve.proxmox.com/pve-docs/pvecm.1.html</p>
<h5 id="ansible-task-to-re-link-symlinks">Ansible task to re-link symlinks<a class="headerlink" href="#ansible-task-to-re-link-symlinks" title="Permanent link">&para;</a></h5>
<p>Here is my ansible task, which corrects issues with symlinks</p>
<div class="highlight"><pre><span></span><code><span class="nn">---</span>
<span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Check if symbolic link exists</span>
<span class="w">  </span><span class="nt">stat</span><span class="p">:</span>
<span class="w">    </span><span class="nt">path</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">/etc/ssh/ssh_known_hosts</span>
<span class="w">  </span><span class="nt">register</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">ssh_known_hosts_stat</span>

<span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Remove existing file if it&#39;s not a symbolic link</span>
<span class="w">  </span><span class="nt">file</span><span class="p">:</span>
<span class="w">    </span><span class="nt">path</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">/etc/ssh/ssh_known_hosts</span>
<span class="w">    </span><span class="nt">state</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">absent</span>
<span class="w">  </span><span class="nt">when</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">ssh_known_hosts_stat.stat.islnk == false</span>

<span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Link /etc/ssh/ssh_known_hosts</span>
<span class="w">  </span><span class="nt">file</span><span class="p">:</span>
<span class="w">    </span><span class="nt">src</span><span class="p">:</span><span class="w">  </span><span class="l l-Scalar l-Scalar-Plain">/etc/pve/priv/known_hosts</span>
<span class="w">    </span><span class="nt">dest</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">/etc/ssh/ssh_known_hosts</span>
<span class="w">    </span><span class="nt">state</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">link</span>
</code></pre></div>
<p>After completing this step- the errors will go away.</p>
<h2 id="summary">Summary<a class="headerlink" href="#summary" title="Permanent link">&para;</a></h2>
<p>If- you are going through this process- hopefully the above documentation should save you a lot of time.</p>
<p>It personally took me two or three hours to get this node fully back online and functional, as these steps, aren't really documented anywhere I could find.</p>
<p>A few random links which were less then helpful...</p>
<ul>
<li>https://forum.proxmox.com/threads/backup-pve-host-and-restore-to-new-disk.129839/<ul>
<li>Nothing of use in here</li>
</ul>
</li>
<li>https://www.reddit.com/r/Proxmox/comments/105jqwp/backing_up_proxmox_host/<ul>
<li>nothing of use in here either.</li>
</ul>
</li>
<li>https://forum.proxmox.com/threads/backup-restore-procedure-for-proxmox-ve-host.118264/</li>
<li>https://forum.proxmox.com/threads/how-to-restore-pve-host.139585/<ul>
<li>had a few good pointers</li>
</ul>
</li>
<li>https://pve.proxmox.com/wiki/Proxmox_Cluster_File_System_(pmxcfs)#_recovery<ul>
<li>useless</li>
</ul>
</li>
<li>https://pbs.proxmox.com/docs/backup-client.html#creating-backups<ul>
<li>Also, useless. (There is no documentation on how to restore a host from PBS either!)</li>
</ul>
</li>
</ul>
<p>So- on that note, enjoy the first actual documentation (that I could find) on how to successfully reinstall a failed cluster node, and re-importing its configuration.</p>
<p>I will note, this also validates that my backup strategy of backing up the <code>/etc/pve</code> directory from a single host, is adequate for doing a cluster recovery, if needed.</p>







  
  




  



      
    </article>
  </div>

          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg>
  Back to top
</button>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
    <div class="md-copyright__highlight">
      Copyright &copy; 2025 XtremeOwnage.com
    </div>
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs Insiders
    </a>
  
</div>
      
        <div class="md-social">
  
    
    
    
    
      
      
    
    <a href="https://github.com/XtremeOwnageDotCom" target="_blank" rel="noopener" title="github.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><!--! Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8M97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg>
    </a>
  
    
    
    
    
    <a href="/discord" target="_blank" rel="noopener" title="" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 640 512"><!--! Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M524.531 69.836a1.5 1.5 0 0 0-.764-.7A485 485 0 0 0 404.081 32.03a1.82 1.82 0 0 0-1.923.91 338 338 0 0 0-14.9 30.6 447.9 447.9 0 0 0-134.426 0 310 310 0 0 0-15.135-30.6 1.89 1.89 0 0 0-1.924-.91 483.7 483.7 0 0 0-119.688 37.107 1.7 1.7 0 0 0-.788.676C39.068 183.651 18.186 294.69 28.43 404.354a2.02 2.02 0 0 0 .765 1.375 487.7 487.7 0 0 0 146.825 74.189 1.9 1.9 0 0 0 2.063-.676A348 348 0 0 0 208.12 430.4a1.86 1.86 0 0 0-1.019-2.588 321 321 0 0 1-45.868-21.853 1.885 1.885 0 0 1-.185-3.126 251 251 0 0 0 9.109-7.137 1.82 1.82 0 0 1 1.9-.256c96.229 43.917 200.41 43.917 295.5 0a1.81 1.81 0 0 1 1.924.233 235 235 0 0 0 9.132 7.16 1.884 1.884 0 0 1-.162 3.126 301.4 301.4 0 0 1-45.89 21.83 1.875 1.875 0 0 0-1 2.611 391 391 0 0 0 30.014 48.815 1.86 1.86 0 0 0 2.063.7A486 486 0 0 0 610.7 405.729a1.88 1.88 0 0 0 .765-1.352c12.264-126.783-20.532-236.912-86.934-334.541M222.491 337.58c-28.972 0-52.844-26.587-52.844-59.239s23.409-59.241 52.844-59.241c29.665 0 53.306 26.82 52.843 59.239 0 32.654-23.41 59.241-52.843 59.241m195.38 0c-28.971 0-52.843-26.587-52.843-59.239s23.409-59.241 52.843-59.241c29.667 0 53.307 26.82 52.844 59.239 0 32.654-23.177 59.241-52.844 59.241"/></svg>
    </a>
  
    
    
    
    
      
      
    
    <a href="https://www.reddit.com/r/HTTP_404_NotFound" target="_blank" rel="noopener" title="www.reddit.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M0 256C0 114.6 114.6 0 256 0s256 114.6 256 256-114.6 256-256 256H37.1c-13.7 0-20.5-16.5-10.9-26.2L75 437C28.7 390.7 0 326.7 0 256m349.6-102.4c23.6 0 42.7-19.1 42.7-42.7s-19.1-42.7-42.7-42.7c-20.6 0-37.8 14.6-41.8 34-34.5 3.7-61.4 33-61.4 68.4v.2c-37.5 1.6-71.8 12.3-99 29.1-10.1-7.8-22.8-12.5-36.5-12.5-33 0-59.8 26.8-59.8 59.8 0 24 14.1 44.6 34.4 54.1 2 69.4 77.6 125.2 170.6 125.2s168.7-55.9 170.6-125.3c20.2-9.6 34.1-30.2 34.1-54 0-33-26.8-59.8-59.8-59.8-13.7 0-26.3 4.6-36.4 12.4-27.4-17-62.1-27.7-100-29.1v-.2c0-25.4 18.9-46.5 43.4-49.9 4.4 18.8 21.3 32.8 41.5 32.8zm-172.5 93.3c16.7 0 29.5 17.6 28.5 39.3s-13.5 29.6-30.3 29.6-31.4-8.8-30.4-30.5S160.3 247 177 247zm190.1 38.3c1 21.7-13.7 30.5-30.4 30.5s-29.3-7.9-30.3-29.6 11.8-39.3 28.5-39.3 31.2 16.6 32.1 38.3zm-48.1 56.7c-10.3 24.6-34.6 41.9-63 41.9s-52.7-17.3-63-41.9c-1.2-2.9.8-6.2 3.9-6.5 18.4-1.9 38.3-2.9 59.1-2.9s40.7 1 59.1 2.9c3.1.3 5.1 3.6 3.9 6.5"/></svg>
    </a>
  
    
    
    
    
      
      
    
    <a href="https://static.xtremeownage.com/" target="_blank" rel="noopener" title="static.xtremeownage.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M352 256c0 22.2-1.2 43.6-3.3 64H163.4c-2.2-20.4-3.3-41.8-3.3-64s1.2-43.6 3.3-64h185.3c2.2 20.4 3.3 41.8 3.3 64m28.8-64h123.1c5.3 20.5 8.1 41.9 8.1 64s-2.8 43.5-8.1 64H380.8c2.1-20.6 3.2-42 3.2-64s-1.1-43.4-3.2-64m112.6-32H376.7c-10-63.9-29.8-117.4-55.3-151.6 78.3 20.7 142 77.5 171.9 151.6zm-149.1 0H167.7c6.1-36.4 15.5-68.6 27-94.7 10.5-23.6 22.2-40.7 33.5-51.5C239.4 3.2 248.7 0 256 0s16.6 3.2 27.8 13.8c11.3 10.8 23 27.9 33.5 51.5 11.6 26 20.9 58.2 27 94.7m-209 0H18.6c30-74.1 93.6-130.9 172-151.6-25.5 34.2-45.3 87.7-55.3 151.6M8.1 192h123.1c-2.1 20.6-3.2 42-3.2 64s1.1 43.4 3.2 64H8.1C2.8 299.5 0 278.1 0 256s2.8-43.5 8.1-64m186.6 254.6c-11.6-26-20.9-58.2-27-94.6h176.6c-6.1 36.4-15.5 68.6-27 94.6-10.5 23.6-22.2 40.7-33.5 51.5-11.2 10.7-20.5 13.9-27.8 13.9s-16.6-3.2-27.8-13.8c-11.3-10.8-23-27.9-33.5-51.5zM135.3 352c10 63.9 29.8 117.4 55.3 151.6-78.4-20.7-142-77.5-172-151.6zm358.1 0c-30 74.1-93.6 130.9-171.9 151.6 25.5-34.2 45.2-87.7 55.3-151.6h116.7z"/></svg>
    </a>
  
    
    
    
    
    <a href="/feed_rss_created.xml" target="_blank" rel="noopener" title="" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M0 64c0-17.7 14.3-32 32-32 229.8 0 416 186.2 416 416 0 17.7-14.3 32-32 32s-32-14.3-32-32C384 253.6 226.4 96 32 96 14.3 96 0 81.7 0 64m0 352a64 64 0 1 1 128 0 64 64 0 1 1-128 0m32-256c159.1 0 288 128.9 288 288 0 17.7-14.3 32-32 32s-32-14.3-32-32c0-123.7-100.3-224-224-224-17.7 0-32-14.3-32-32s14.3-32 32-32"/></svg>
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      
      <script id="__config" type="application/json">{"annotate": null, "base": "../../..", "features": ["navigation.tabs", "navigation.tabs.sticky", "navigation.top", "navigation.instant", "navigation.tracking", "navigation.expand", "navigation.indexes", "toc.follow", "search.suggest", "search.highlight", "search.share", "content.footnote.tooltips", "content.code.copy", "content.code.select", "content.code.annotate"], "search": "../../../assets/javascripts/workers/search.ad38c271.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script>
    
    
      <script src="../../../assets/javascripts/bundle.eb438620.min.js"></script>
      
    
  <script id="init-glightbox">const lightbox = GLightbox({"touchNavigation": true, "loop": false, "zoomable": true, "draggable": true, "openEffect": "zoom", "closeEffect": "zoom", "slideEffect": "slide"});
document$.subscribe(() => { lightbox.reload() });
</script></body>
</html>